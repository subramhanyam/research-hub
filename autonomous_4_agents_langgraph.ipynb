{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62ff70b7",
   "metadata": {},
   "source": [
    "# 4 Autonomous Research Agents ‚Äî LangGraph Agentic Graph\n",
    "\n",
    "**Architecture:** A single LangGraph executor node routes dynamically at runtime.  \n",
    "The flow is **never hard-coded** ‚Äî each agent decides `next_agent` in its output.\n",
    "\n",
    "| Agent | Role |\n",
    "|-------|------|\n",
    "| **Planner** | Understands goal, breaks into sub-questions, decides who runs next |\n",
    "| **Researcher** | RAG retrieval from vector DB, grounded answers + sources |\n",
    "| **Evaluator** | Scores confidence, detects gaps, routes to Expansion or END |\n",
    "| **Expansion** | ArXiv search (online mode) **or** idea generation only (workspace mode) |\n",
    "\n",
    "---\n",
    "\n",
    "## Two Operating Modes\n",
    "\n",
    "| Mode | `search_mode` value | Expansion behaviour |\n",
    "|------|--------------------|--------------------|\n",
    "| **Online** | `\"online\"` | Searches ArXiv, downloads & indexes new papers, loops back to Researcher |\n",
    "| **Workspace-only** | `\"workspace\"` | Never touches the internet ‚Äî uses only local PDFs already indexed |\n",
    "\n",
    "Set `SEARCH_MODE` in **Section 11** before running.\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "  ‚îÇ          LangGraph StateGraph           ‚îÇ\n",
    "  ‚îÇ                                         ‚îÇ\n",
    "  ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ\n",
    "  ‚îÇ   ‚îÇ       agent_executor         ‚îÇ‚óÑ‚îÄ‚îê   ‚îÇ\n",
    "  ‚îÇ   ‚îÇ  (single dynamic dispatcher) ‚îÇ  ‚îÇ   ‚îÇ\n",
    "  ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ   ‚îÇ\n",
    "  ‚îÇ              ‚îÇ                      ‚îÇ   ‚îÇ\n",
    "  ‚îÇ         route(state)                ‚îÇ   ‚îÇ\n",
    "  ‚îÇ         /         \\                 ‚îÇ   ‚îÇ\n",
    "  ‚îÇ      LOOP          END              ‚îÇ   ‚îÇ\n",
    "  ‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ\n",
    "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "  AGENT_REGISTRY maps name ‚Üí function at runtime:\n",
    "    'Planner'    ‚Üí planner_agent(state)\n",
    "    'Researcher' ‚Üí researcher_agent(state)\n",
    "    'Evaluator'  ‚Üí evaluator_agent(state)\n",
    "    'Expansion'  ‚Üí expansion_agent(state)   ‚Üê behaviour controlled by search_mode\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88b4da2",
   "metadata": {},
   "source": [
    "## 0. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6442b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q langgraph langchain langchain-core langchain-community langchain-groq \\\n",
    "#               langchain-huggingface langchain-chroma chromadb sentence-transformers \\\n",
    "#               pypdf arxiv python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c28b3ba",
   "metadata": {},
   "source": [
    "## 1. Imports & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afbfd5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\S\\1602_24_733_186\\Reasearch Hub\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM ready: llama-3.3-70b-versatile\n",
      "üìÅ Workspace: D:\\S\\1602_24_733_186\\Reasearch Hub\\agent_workspace\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, TypedDict\n",
    "\n",
    "import arxiv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ‚îÄ‚îÄ LLM (Groq ‚Äî swap for any LangChain ChatModel) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "llm = None\n",
    "try:\n",
    "    from langchain_groq import ChatGroq\n",
    "    _key = os.getenv(\"GROQ_API_KEY\", \"\")\n",
    "    if _key:\n",
    "        llm = ChatGroq(\n",
    "            model=os.getenv(\"GROQ_MODEL\", \"llama-3.3-70b-versatile\"),\n",
    "            temperature=0.1,\n",
    "            api_key=_key,\n",
    "        )\n",
    "        print(\"‚úÖ LLM ready:\", llm.model_name)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  GROQ_API_KEY not set ‚Äî agents will use fallback logic\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  langchain-groq not installed ‚Äî agents will use fallback logic\")\n",
    "\n",
    "# ‚îÄ‚îÄ Workspace directories ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "WORK_DIR   = Path(\"./agent_workspace\")\n",
    "PDF_DIR    = WORK_DIR / \"papers_local\"       # drop your PDFs here\n",
    "ARXIV_DIR  = WORK_DIR / \"papers_arxiv\"       # Expansion agent saves here\n",
    "CHROMA_DIR = WORK_DIR / \"chroma_db\"\n",
    "\n",
    "for d in [WORK_DIR, PDF_DIR, ARXIV_DIR, CHROMA_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"üìÅ Workspace:\", WORK_DIR.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11c34cc",
   "metadata": {},
   "source": [
    "## 2. Shared Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bee05c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helpers ready\n"
     ]
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ JSON parsing ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def _parse_json(text: str) -> dict:\n",
    "    t = text.strip()\n",
    "    if t.startswith(\"```\"):\n",
    "        t = t.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "    return json.loads(t)\n",
    "\n",
    "\n",
    "def _llm_json(prompt: str, fallback: dict) -> dict:\n",
    "    \"\"\"Call the LLM and parse JSON output. Return fallback on any error.\"\"\"\n",
    "    if llm is None:\n",
    "        return fallback\n",
    "    try:\n",
    "        raw = llm.invoke([HumanMessage(content=prompt)]).content\n",
    "        return _parse_json(raw)\n",
    "    except Exception as e:\n",
    "        print(f\"  [LLM parse error] {e}\")\n",
    "        return fallback\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ Vector DB (Chroma + HuggingFace embeddings) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def build_vectordb():\n",
    "    \"\"\"Instantiate (or reuse) the persistent Chroma vector store.\"\"\"\n",
    "    try:\n",
    "        from langchain_huggingface import HuggingFaceEmbeddings\n",
    "    except ImportError:\n",
    "        from langchain_community.embeddings import HuggingFaceEmbeddings  # type: ignore\n",
    "    try:\n",
    "        from langchain_chroma import Chroma\n",
    "    except ImportError:\n",
    "        from langchain_community.vectorstores import Chroma  # type: ignore\n",
    "\n",
    "    emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vdb = Chroma(\n",
    "        collection_name=\"autonomous_4agents\",\n",
    "        embedding_function=emb,\n",
    "        persist_directory=str(CHROMA_DIR),\n",
    "    )\n",
    "    return vdb\n",
    "\n",
    "\n",
    "def ingest_dirs(vdb, dirs: list[Path]) -> int:\n",
    "    \"\"\"Load all PDFs from dirs, chunk them, and upsert into the vector store.\"\"\"\n",
    "    pdfs = [p for d in dirs for p in d.rglob(\"*.pdf\")]\n",
    "    docs: list[Document] = []\n",
    "    for p in pdfs:\n",
    "        try:\n",
    "            pages = PyPDFLoader(str(p)).load()\n",
    "            for pg in pages:\n",
    "                pg.metadata[\"source\"] = str(p)\n",
    "            docs.extend(pages)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not docs:\n",
    "        return 0\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1_200, chunk_overlap=200)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    ids = [\n",
    "        hashlib.md5(\n",
    "            f\"{c.metadata.get('source')}|{c.metadata.get('page',-1)}|{c.page_content[:80]}\"\n",
    "            .encode()\n",
    "        ).hexdigest()\n",
    "        for c in chunks\n",
    "    ]\n",
    "    if chunks:\n",
    "        vdb.add_documents(chunks, ids=ids)\n",
    "    return len(chunks)\n",
    "\n",
    "\n",
    "def retrieve(vdb, query: str, k: int = 8) -> list[Document]:\n",
    "    return vdb.similarity_search(query, k=k)\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ ArXiv helper ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def arxiv_expand(query: str, max_results: int = 4) -> list[Path]:\n",
    "    \"\"\"Search ArXiv, download PDFs, return file paths.\"\"\"\n",
    "    client = arxiv.Client(page_size=max_results, delay_seconds=3, num_retries=3)\n",
    "    search = arxiv.Search(\n",
    "        query=query, max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance,\n",
    "    )\n",
    "    saved = []\n",
    "    for r in client.results(search):\n",
    "        try:\n",
    "            fp = r.download_pdf(dirpath=str(ARXIV_DIR), filename=f\"{r.get_short_id()}.pdf\")\n",
    "            saved.append(Path(fp))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return saved\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helpers ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d36827",
   "metadata": {},
   "source": [
    "## 3. State Schema\n",
    "\n",
    "One `TypedDict` carries everything through the graph.  \n",
    "`current_agent` tells the executor **which** agent to run next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2698e88e",
   "metadata": {},
   "outputs": [],
   "source": "class AgentState(TypedDict, total=False):\n    # ‚îÄ‚îÄ Core ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n    goal: str                   # research question\n    vdb: Any                    # Chroma vector store (passed by reference)\n    current_agent: str          # which agent is active RIGHT NOW\n    next_agent: str             # where to route after this agent\n    iterations: int             # expansion loop counter\n    max_iterations: int         # hard cap on expansion loops\n\n    # ‚îÄ‚îÄ Mode ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n    search_mode: str            # \"online\"  ‚Üí ArXiv search allowed\n                                # \"workspace\" ‚Üí use only local/already-indexed PDFs\n\n    # ‚îÄ‚îÄ Per-agent outputs (accumulated in state) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n    planner:    dict            # {subtasks, next_agent}\n    researcher: dict            # {answer, sources, next_agent}\n    evaluator:  dict            # {confidence, missing_aspects, next_agent}\n    expansion:  dict            # {query, papers_downloaded, chunks_added, saturated, ideas, mode}\n\n    # ‚îÄ‚îÄ Final outputs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n    final_answer: str\n    final_sources: list\n    future_ideas: list\n    trace: list                 # execution log [\"Planner ‚Üí Researcher\", ...]\n\n    # ‚îÄ‚îÄ Insufficiency tracking ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n    insufficient_data: bool     # True when the system could not gather enough evidence\n    insufficiency_reason: str   # human-readable explanation of why data was insufficient"
  },
  {
   "cell_type": "markdown",
   "id": "fd276948",
   "metadata": {},
   "source": [
    "## 4. Agent 1 ‚Äî Planner (Lightweight Brain)\n",
    "\n",
    "- Understands the research goal  \n",
    "- Breaks it into sub-questions  \n",
    "- Decides who runs next (always `Researcher`)  \n",
    "- **Does NOT search** ‚Äî pure reasoning only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ac817b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def planner_agent(state: AgentState) -> dict:\n",
    "    goal = state[\"goal\"]\n",
    "    print(f\"  üß† [Planner] Goal: {goal[:80]}...\")\n",
    "\n",
    "    fallback = {\n",
    "        \"subtasks\": [\"model architecture\", \"training datasets\", \"evaluation benchmarks\"],\n",
    "        \"next_agent\": \"Researcher\",\n",
    "    }\n",
    "    prompt = f\"\"\"You are the Planner Agent.\n",
    "Research goal: {goal}\n",
    "\n",
    "Your job:\n",
    "1. Understand the goal deeply.\n",
    "2. Break it into 3-5 focused sub-questions.\n",
    "3. Do NOT search ‚Äî only plan.\n",
    "4. Always route next to Researcher.\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\"subtasks\": [\"sub-question 1\", \"sub-question 2\", \"...\"], \"next_agent\": \"Researcher\"}}\"\"\"\n",
    "\n",
    "    out = _llm_json(prompt, fallback)\n",
    "    subtasks = out.get(\"subtasks\", fallback[\"subtasks\"])\n",
    "    if not isinstance(subtasks, list):\n",
    "        subtasks = fallback[\"subtasks\"]\n",
    "\n",
    "    print(f\"  üß† [Planner] Subtasks: {subtasks}\")\n",
    "    return {\n",
    "        \"planner\": {\"subtasks\": subtasks, \"next_agent\": \"Researcher\"},\n",
    "        \"next_agent\": \"Researcher\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f53e74f",
   "metadata": {},
   "source": [
    "## 5. Agent 2 ‚Äî Researcher (RAG Agent)\n",
    "\n",
    "- Retrieves evidence from the Chroma vector DB  \n",
    "- Generates a grounded answer with source citations  \n",
    "- Routes to `Evaluator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c27e407",
   "metadata": {},
   "outputs": [],
   "source": "def researcher_agent(state: AgentState) -> dict:\n    goal     = state[\"goal\"]\n    vdb      = state[\"vdb\"]\n    subtasks = state.get(\"planner\", {}).get(\"subtasks\", [])\n    mode     = state.get(\"search_mode\", \"online\")\n    print(f\"  üîç [Researcher] Retrieving for {len(subtasks)} subtasks...\")\n\n    # RAG retrieval\n    query = f\"Goal: {goal}\\nSubtasks: {'; '.join(subtasks)}\"\n    docs  = retrieve(vdb, query, k=8)\n\n    sources  = []\n    evidence = []\n    for d in docs:\n        src  = d.metadata.get(\"source\", \"unknown\")\n        page = d.metadata.get(\"page\", \"?\")\n        sources.append({\"source\": src, \"page\": page})\n        evidence.append(f\"[{src} | page {page}]\\n{d.page_content[:800]}\")\n\n    # ‚îÄ‚îÄ No evidence in DB ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n    if not evidence:\n        if mode == \"online\":\n            reason = (\n                \"The vector DB is empty ‚Äî no local or ArXiv papers have been indexed yet. \"\n                \"The Expansion agent will search ArXiv.\"\n            )\n        else:\n            reason = (\n                \"The vector DB is empty and workspace mode is active ‚Äî \"\n                \"no local PDFs were found. \"\n                f\"Please add PDFs to: {PDF_DIR.resolve()} and re-run Section 10.\"\n            )\n        print(f\"  üîç [Researcher] ‚ö†Ô∏è  No evidence ‚Äî {reason}\")\n        return {\n            \"researcher\":        {\"answer\": \"\", \"sources\": [], \"next_agent\": \"Evaluator\"},\n            \"final_answer\":      \"\",\n            \"final_sources\":     [],\n            \"insufficient_data\": True,\n            \"insufficiency_reason\": reason,\n            \"next_agent\":        \"Evaluator\",\n        }\n\n    # ‚îÄ‚îÄ Build grounded answer from evidence ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n    fallback = {\"answer\": \"Insufficient grounded evidence.\", \"next_agent\": \"Evaluator\"}\n    prompt = f\"\"\"You are the Researcher Agent (RAG).\nResearch goal: {goal}\nSubtasks: {subtasks}\n\nRetrieved evidence:\n{chr(10).join(evidence[:6])}\n\nWrite a grounded, factual answer citing evidence above.\nReturn ONLY valid JSON:\n{{\"answer\": \"...\", \"next_agent\": \"Evaluator\"}}\"\"\"\n    out    = _llm_json(prompt, fallback)\n    answer = str(out.get(\"answer\", fallback[\"answer\"]))\n\n    print(f\"  üîç [Researcher] Sources found: {len(sources)}\")\n    return {\n        \"researcher\":    {\"answer\": answer, \"sources\": sources, \"next_agent\": \"Evaluator\"},\n        \"final_answer\":  answer,\n        \"final_sources\": sources,\n        \"next_agent\":    \"Evaluator\",\n    }"
  },
  {
   "cell_type": "markdown",
   "id": "9a354826",
   "metadata": {},
   "source": [
    "## 6. Agent 3 ‚Äî Evaluator (Critic + Analyzer)\n",
    "\n",
    "- Judges whether the answer is sufficient  \n",
    "- Scores confidence (0‚Äì1)  \n",
    "- Detects missing aspects  \n",
    "- **Online mode:** routes to `END` if confidence ‚â• 0.75 **and** ‚â• 4 sources, else `Expansion`  \n",
    "- **Workspace mode:** routes to `END` once the vector DB has been exhausted (no new chunks possible), accepting a lower confidence threshold (‚â• 0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72bab78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_agent(state: AgentState) -> dict:\n",
    "    goal        = state[\"goal\"]\n",
    "    answer      = state.get(\"researcher\", {}).get(\"answer\", \"\")\n",
    "    sources     = state.get(\"researcher\", {}).get(\"sources\", [])\n",
    "    mode        = state.get(\"search_mode\", \"online\")\n",
    "    print(f\"  ‚öñÔ∏è  [Evaluator] Mode={mode} | Judging answer ({len(sources)} sources)...\")\n",
    "\n",
    "    fallback = {\n",
    "        \"confidence\": 0.55 if len(sources) >= 3 else 0.30,\n",
    "        \"missing_aspects\": [\"experimental validation\", \"ablation studies\"],\n",
    "        \"next_agent\": \"Expansion\",\n",
    "    }\n",
    "\n",
    "    mode_instruction = (\n",
    "        \"If confidence >= 0.75 AND sources >= 4, set next_agent to END. \"\n",
    "        \"Otherwise set next_agent to Expansion.\"\n",
    "        if mode == \"online\"\n",
    "        else\n",
    "        \"If confidence >= 0.50 OR sources >= 2, set next_agent to END. \"\n",
    "        \"We are in workspace-only mode ‚Äî no new papers can be fetched, \"\n",
    "        \"so Expansion will only generate ideas, not improve retrieval. \"\n",
    "        \"Be lenient: end unless the answer is clearly empty or completely off-topic.\"\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"You are the Evaluator Agent.\n",
    "Research goal: {goal}\n",
    "Current answer: {answer[:1500]}\n",
    "Number of sources: {len(sources)}\n",
    "Operating mode: {mode}\n",
    "\n",
    "Tasks:\n",
    "1. Score confidence from 0.0 to 1.0 (how well the answer covers the goal).\n",
    "2. List specific missing aspects (if any).\n",
    "3. Routing rule ‚Äî {mode_instruction}\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\"confidence\": 0.62, \"missing_aspects\": [\"...\"], \"next_agent\": \"Expansion\"}}\"\"\"\n",
    "\n",
    "    out = _llm_json(prompt, fallback)\n",
    "\n",
    "    confidence      = float(out.get(\"confidence\", fallback[\"confidence\"]))\n",
    "    missing_aspects = out.get(\"missing_aspects\", fallback[\"missing_aspects\"])\n",
    "    if not isinstance(missing_aspects, list):\n",
    "        missing_aspects = fallback[\"missing_aspects\"]\n",
    "\n",
    "    # Hard sufficiency rule ‚Äî differs by mode\n",
    "    if mode == \"workspace\":\n",
    "        # In workspace mode be lenient: end if we have any answer with sources,\n",
    "        # or if confidence is reasonable. Expansion can only generate ideas anyway.\n",
    "        sufficient = (confidence >= 0.50) or (len(sources) >= 2)\n",
    "    else:\n",
    "        sufficient = confidence >= 0.75 and len(sources) >= 4\n",
    "\n",
    "    next_agent = \"END\" if sufficient else \"Expansion\"\n",
    "\n",
    "    print(f\"  ‚öñÔ∏è  [Evaluator] Confidence={confidence:.2f} | Sufficient={sufficient} | Next={next_agent}\")\n",
    "    return {\n",
    "        \"evaluator\": {\n",
    "            \"confidence\":      confidence,\n",
    "            \"missing_aspects\": missing_aspects,\n",
    "            \"next_agent\":      next_agent,\n",
    "        },\n",
    "        \"next_agent\": next_agent,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee34db",
   "metadata": {},
   "source": [
    "## 7. Agent 4 ‚Äî Expansion (Explorer + Innovator)\n",
    "\n",
    "**Online mode (`search_mode = \"online\"`):**\n",
    "- Searches ArXiv for papers covering missing aspects  \n",
    "- Ingests new PDFs into the vector DB  \n",
    "- If new papers found ‚Üí loops back to `Researcher`  \n",
    "- If saturated (no new papers **or** max iterations reached) ‚Üí generates ideas ‚Üí `END`\n",
    "\n",
    "**Workspace-only mode (`search_mode = \"workspace\"`):**\n",
    "- Skips all internet access  \n",
    "- Goes straight to generating novel research ideas from missing aspects  \n",
    "- Always routes to `END` (no new retrieval is possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92245aa9",
   "metadata": {},
   "outputs": [],
   "source": "def expansion_agent(state: AgentState) -> dict:\n    goal           = state[\"goal\"]\n    vdb            = state[\"vdb\"]\n    missing        = state.get(\"evaluator\", {}).get(\"missing_aspects\", [])\n    iterations     = int(state.get(\"iterations\", 0)) + 1\n    max_iterations = int(state.get(\"max_iterations\", 4))\n    mode           = state.get(\"search_mode\", \"online\")\n    # Was the last Researcher call a total blank?\n    prev_answer    = state.get(\"final_answer\", \"\")\n    prev_sources   = state.get(\"final_sources\", [])\n\n    print(f\"  üöÄ [Expansion] Mode={mode} | Iteration {iterations}/{max_iterations}\")\n    print(f\"  üöÄ [Expansion] Missing aspects: {missing}\")\n\n    # ‚îÄ‚îÄ Shared: generate novel research ideas ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n    def _generate_ideas(reason: str) -> list[str]:\n        print(f\"  üöÄ [Expansion] Generating ideas ({reason})...\")\n        fallback_ideas = {\n            \"ideas\": [\n                \"Design targeted experiments to address missing aspects.\",\n                \"Create stronger benchmarks with edge-case coverage.\",\n                \"Run ablation studies to isolate causal contributors.\",\n                \"Apply iterative human-in-the-loop error analysis.\",\n                \"Explore cross-domain transfer to validate generalisability.\",\n            ]\n        }\n        prompt = f\"\"\"You are the Expansion Agent.\nResearch goal: {goal}\nMissing aspects: {missing}\nReason for idea generation: {reason}\n\nGenerate 4-6 concrete, novel research ideas or future directions\nthat could address the missing aspects.\n\nReturn ONLY valid JSON:\n{{\"ideas\": [\"idea 1\", \"idea 2\", \"...\"]}}\"\"\"\n        out = _llm_json(prompt, fallback_ideas)\n        raw = out.get(\"ideas\", fallback_ideas[\"ideas\"])\n        return [str(x) for x in raw] if isinstance(raw, list) else fallback_ideas[\"ideas\"]\n\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    # WORKSPACE-ONLY MODE ‚Äî no internet, ideas only\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    if mode == \"workspace\":\n        ideas = _generate_ideas(\"workspace-only mode ‚Äî no ArXiv access\")\n\n        # Detect true insufficiency: DB was empty when Researcher ran\n        truly_insufficient = (len(prev_sources) == 0 and prev_answer == \"\")\n        if truly_insufficient:\n            insuff_reason = (\n                \"Workspace mode is active but the vector DB contains no documents. \"\n                f\"Add PDFs to {PDF_DIR.resolve()} and re-run Section 10.\"\n            )\n            print(f\"  üöÄ [Expansion] ‚ö†Ô∏è  INSUFFICIENT DATA ‚Äî {insuff_reason}\")\n        else:\n            insuff_reason = \"\"\n\n        print(f\"  üöÄ [Expansion] [WORKSPACE] Generated {len(ideas)} ideas ‚Üí END\")\n        out_dict = {\n            \"iterations\": iterations,\n            \"expansion\": {\n                \"mode\":              \"workspace\",\n                \"query\":             None,\n                \"papers_downloaded\": 0,\n                \"chunks_added\":      0,\n                \"saturated\":         True,\n                \"ideas\":             ideas,\n                \"next_agent\":        \"END\",\n            },\n            \"future_ideas\": ideas,\n            \"next_agent\":   \"END\",\n        }\n        if truly_insufficient:\n            out_dict[\"insufficient_data\"]     = True\n            out_dict[\"insufficiency_reason\"]  = insuff_reason\n        return out_dict\n\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    # ONLINE MODE ‚Äî ArXiv search + ingest\n    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n    query      = f\"{goal} {' '.join(missing[:3])}\"\n    downloaded = arxiv_expand(query, max_results=4)\n    added      = ingest_dirs(vdb, [ARXIV_DIR]) if downloaded else 0\n    print(f\"  üöÄ [Expansion] [ONLINE] Downloaded {len(downloaded)} papers | Indexed {added} chunks\")\n\n    saturated  = (added == 0) or (iterations >= max_iterations)\n    ideas      = _generate_ideas(\"ArXiv saturated\") if saturated else []\n    next_agent = \"END\" if saturated else \"Researcher\"\n\n    # Detect true insufficiency: saturated on iteration 1 with no prior answer\n    truly_insufficient = (\n        saturated\n        and iterations == 1\n        and len(downloaded) == 0\n        and len(prev_sources) == 0\n    )\n    if truly_insufficient:\n        insuff_reason = (\n            f\"ArXiv returned no papers for the query: '{query}'. \"\n            \"The topic may be too niche, phrased unusually, or ArXiv may be unreachable. \"\n            \"Try rephrasing the research goal, adding local PDFs, or checking your connection.\"\n        )\n        print(f\"  üöÄ [Expansion] ‚ö†Ô∏è  INSUFFICIENT DATA ‚Äî {insuff_reason}\")\n    else:\n        insuff_reason = \"\"\n\n    print(f\"  üöÄ [Expansion] [ONLINE] Saturated={saturated} | Next={next_agent}\")\n    out_dict = {\n        \"iterations\": iterations,\n        \"expansion\": {\n            \"mode\":              \"online\",\n            \"query\":             query,\n            \"papers_downloaded\": len(downloaded),\n            \"chunks_added\":      added,\n            \"saturated\":         saturated,\n            \"ideas\":             ideas,\n            \"next_agent\":        next_agent,\n        },\n        \"future_ideas\": ideas,\n        \"next_agent\":   next_agent,\n    }\n    if truly_insufficient:\n        out_dict[\"insufficient_data\"]    = True\n        out_dict[\"insufficiency_reason\"] = insuff_reason\n    return out_dict"
  },
  {
   "cell_type": "markdown",
   "id": "83165faf",
   "metadata": {},
   "source": [
    "## 8. Dynamic Registry & Executor\n",
    "\n",
    "**No hard-coded edges between agents.**  \n",
    "A single `agent_executor` node reads `state['current_agent']`,  \n",
    "looks up the function in `AGENT_REGISTRY`, and calls it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57432e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Registry: ['Planner', 'Researcher', 'Evaluator', 'Expansion']\n"
     ]
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ Registry: name ‚Üí function ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "AGENT_REGISTRY: dict[str, Callable[[AgentState], dict]] = {\n",
    "    \"Planner\":    planner_agent,\n",
    "    \"Researcher\": researcher_agent,\n",
    "    \"Evaluator\":  evaluator_agent,\n",
    "    \"Expansion\":  expansion_agent,\n",
    "}\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ Single dynamic executor node ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def agent_executor(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    The ONLY node in the graph.\n",
    "    Reads current_agent from state, dispatches to the right function,\n",
    "    then updates state with the result.\n",
    "    \"\"\"\n",
    "    current = state.get(\"current_agent\", \"Planner\")\n",
    "    fn      = AGENT_REGISTRY.get(current)\n",
    "\n",
    "    if fn is None:\n",
    "        print(f\"  ‚ùå Unknown agent: {current}\")\n",
    "        return {\n",
    "            **state,\n",
    "            \"next_agent\":    \"END\",\n",
    "            \"current_agent\": \"END\",\n",
    "            \"trace\":         state.get(\"trace\", []) + [f\"{current} -> UNKNOWN -> END\"],\n",
    "        }\n",
    "\n",
    "    # Run the agent\n",
    "    update     = fn(state)\n",
    "    next_agent = update.get(\"next_agent\", \"END\")\n",
    "    trace      = state.get(\"trace\", []) + [f\"{current} ‚Üí {next_agent}\"]\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        **update,\n",
    "        \"current_agent\": next_agent,\n",
    "        \"trace\":         trace,\n",
    "    }\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ Conditional routing function ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def route(state: AgentState) -> str:\n",
    "    \"\"\"\n",
    "    After every agent_executor call:\n",
    "      - next_agent == 'END'  ‚Üí stop the graph\n",
    "      - anything else        ‚Üí loop back to agent_executor\n",
    "    \"\"\"\n",
    "    return \"END\" if state.get(\"next_agent\", \"END\") == \"END\" else \"LOOP\"\n",
    "\n",
    "\n",
    "print(\"‚úÖ Registry:\", list(AGENT_REGISTRY.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c66847d",
   "metadata": {},
   "source": [
    "## 9. Build the LangGraph\n",
    "\n",
    "```\n",
    "  START\n",
    "    ‚îÇ\n",
    "    ‚ñº\n",
    " agent_executor  ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ                           ‚îÇ\n",
    "  route(state)                  ‚îÇ\n",
    "   /      \\                     ‚îÇ\n",
    " END      LOOP ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "Only **one node**, one **conditional edge** ‚Äî routing is 100% data-driven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fd844a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangGraph compiled\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_graph():\n",
    "    g = StateGraph(AgentState)\n",
    "\n",
    "    # Single executor node\n",
    "    g.add_node(\"agent_executor\", agent_executor)\n",
    "\n",
    "    # Entry point\n",
    "    g.set_entry_point(\"agent_executor\")\n",
    "\n",
    "    # Dynamic routing ‚Äî no hard-coded agent-to-agent edges\n",
    "    g.add_conditional_edges(\n",
    "        \"agent_executor\",\n",
    "        route,\n",
    "        {\n",
    "            \"LOOP\": \"agent_executor\",   # run next agent in same node\n",
    "            \"END\":  END,                # terminate graph\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return g.compile()\n",
    "\n",
    "\n",
    "graph = build_graph()\n",
    "print(\"‚úÖ LangGraph compiled\")\n",
    "\n",
    "# Optional: visualise the graph structure\n",
    "try:\n",
    "    from IPython.display import display, Image\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    print(\"  (graph visualisation requires pygraphviz ‚Äî skipping)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6579e5",
   "metadata": {},
   "source": [
    "## 10. Initialise Vector DB & Ingest Local Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e86f04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vector DB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vamshisaikrishnaa\\AppData\\Local\\Temp\\ipykernel_20772\\4140445745.py:33: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:00<00:00, 316.93it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "C:\\Users\\vamshisaikrishnaa\\AppData\\Local\\Temp\\ipykernel_20772\\4140445745.py:34: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  vdb = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector DB ready | 892 chunks indexed from existing PDFs\n",
      "   Drop additional PDFs into: D:\\S\\1602_24_733_186\\Reasearch Hub\\agent_workspace\\papers_local\n"
     ]
    }
   ],
   "source": [
    "print(\"Building vector DB...\")\n",
    "vdb = build_vectordb()\n",
    "\n",
    "# Ingest any PDFs already in the workspace\n",
    "chunks = ingest_dirs(vdb, [PDF_DIR, ARXIV_DIR])\n",
    "print(f\"‚úÖ Vector DB ready | {chunks} chunks indexed from existing PDFs\")\n",
    "print(f\"   Drop additional PDFs into: {PDF_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef47fc4e",
   "metadata": {},
   "source": [
    "## 11. Run the Autonomous Agent System\n",
    "\n",
    "**Choose your mode then run the cell below.**\n",
    "\n",
    "| `SEARCH_MODE` | What happens |\n",
    "|---------------|-------------|\n",
    "| `\"online\"` | Planner ‚Üí Researcher (local RAG) ‚Üí Evaluator ‚Üí **Expansion downloads ArXiv papers** ‚Üí loops until confident ‚Üí ideas |\n",
    "| `\"workspace\"` | Planner ‚Üí Researcher (local RAG only) ‚Üí Evaluator ‚Üí **Expansion generates ideas from existing knowledge** ‚Üí END |\n",
    "\n",
    "> **Tip ‚Äî workspace mode:** Make sure you have PDFs in `./agent_workspace/papers_local/`  \n",
    "> and have run **Section 10** so they are indexed before starting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540965d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "# ‚ïë                    CONFIGURE HERE                           ‚ïë\n",
    "# ‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "# ‚ïë  SEARCH_MODE                                                ‚ïë\n",
    "# ‚ïë    \"online\"    ‚Äî search ArXiv for missing papers            ‚ïë\n",
    "# ‚ïë    \"workspace\" ‚Äî use only locally indexed PDFs              ‚ïë\n",
    "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "SEARCH_MODE    = \"online\"       # ‚Üê change to \"workspace\" for offline mode\n",
    "\n",
    "RESEARCH_GOAL  = (\n",
    "    \"What are the key limitations of large language models in \"\n",
    "    \"long-context reasoning and how can retrieval-augmented generation overcome them?\"\n",
    ")\n",
    "\n",
    "MAX_ITERATIONS = 3              # max Expansion‚ÜíResearcher loops (online mode only)\n",
    "\n",
    "# ‚îÄ‚îÄ Validate mode ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "assert SEARCH_MODE in (\"online\", \"workspace\"), \\\n",
    "    f\"SEARCH_MODE must be 'online' or 'workspace', got: {SEARCH_MODE!r}\"\n",
    "\n",
    "mode_label = {\n",
    "    \"online\":    \"üåê ONLINE  ‚Äî ArXiv search enabled\",\n",
    "    \"workspace\": \"üíæ WORKSPACE ‚Äî local PDFs only, no internet\",\n",
    "}[SEARCH_MODE]\n",
    "\n",
    "# ‚îÄ‚îÄ Initial state ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "initial_state: AgentState = {\n",
    "    \"goal\":           RESEARCH_GOAL,\n",
    "    \"vdb\":            vdb,\n",
    "    \"search_mode\":    SEARCH_MODE,\n",
    "    \"current_agent\":  \"Planner\",\n",
    "    \"next_agent\":     \"Planner\",\n",
    "    \"iterations\":     0,\n",
    "    \"max_iterations\": MAX_ITERATIONS,\n",
    "    \"future_ideas\":   [],\n",
    "    \"trace\":          [],\n",
    "}\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(f\"MODE  : {mode_label}\")\n",
    "print(f\"GOAL  : {RESEARCH_GOAL}\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# ‚îÄ‚îÄ Execute ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "result = graph.invoke(initial_state)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"RUN COMPLETE\")\n",
    "print(\"=\" * 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dafc5c9",
   "metadata": {},
   "source": [
    "## 12. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047cdc62",
   "metadata": {},
   "outputs": [],
   "source": "ex   = result.get(\"expansion\", {})\nmode = result.get(\"search_mode\", \"online\")\n\n# ‚îÄ‚îÄ Header ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nprint(\"=\" * 65)\nprint(f\"MODE : {'üåê ONLINE' if mode == 'online' else 'üíæ WORKSPACE'}\")\nprint(\"=\" * 65)\n\n# ‚îÄ‚îÄ Insufficient data warning ‚Äî shown first if triggered ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nif result.get(\"insufficient_data\"):\n    reason = result.get(\"insufficiency_reason\", \"Unknown reason.\")\n    print(\"\\n\" + \"‚ö†Ô∏è \" * 20)\n    print(\"  INSUFFICIENT DATA ‚Äî the system could not gather enough evidence\")\n    print(\"  Reason  :\", reason)\n    print()\n    if mode == \"online\":\n        print(\"  Suggestions:\")\n        print(\"    1. Rephrase RESEARCH_GOAL with more specific or standard terminology\")\n        print(\"    2. Add relevant PDFs manually to ./agent_workspace/papers_local/\")\n        print(\"       then re-run Section 10 to index them\")\n        print(\"    3. Increase max_results in expansion_agent (currently 4)\")\n        print(\"    4. Check your internet connection (ArXiv requires network access)\")\n    else:\n        print(\"  Suggestions:\")\n        print(\"    1. Add PDFs to ./agent_workspace/papers_local/ and re-run Section 10\")\n        print(\"    2. Switch to SEARCH_MODE = 'online' to let the agent fetch papers\")\n    print(\"‚ö†Ô∏è \" * 20)\n\n# ‚îÄ‚îÄ Execution trace ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nprint(\"\\nEXECUTION TRACE\")\nprint(\"-\" * 40)\nfor step in result.get(\"trace\", []):\n    print(\" ‚Ä¢\", step)\n\n# ‚îÄ‚îÄ Planner output ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nprint(\"\\nPLANNER ‚Äî SUB-QUESTIONS\")\nprint(\"-\" * 40)\nfor i, t in enumerate(result.get(\"planner\", {}).get(\"subtasks\", []), 1):\n    print(f\"  {i}. {t}\")\n\n# ‚îÄ‚îÄ Researcher output ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nprint(\"\\nRESEARCHER ‚Äî GROUNDED ANSWER\")\nprint(\"-\" * 40)\nanswer = result.get(\"final_answer\", \"\")\nif answer:\n    print(answer)\nelse:\n    print(\"  ‚ö†Ô∏è  No answer produced ‚Äî evidence was unavailable.\")\n\nprint(\"\\nRESEARCHER ‚Äî SOURCES\")\nsources = result.get(\"final_sources\", [])\nif sources:\n    for s in sources[:10]:\n        print(f\"  [p.{s.get('page', '?')}] {s.get('source', 'unknown')}\")\nelse:\n    print(\"  ‚ö†Ô∏è  No sources retrieved.\")\n\n# ‚îÄ‚îÄ Evaluator output ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nev = result.get(\"evaluator\", {})\nif ev:\n    print(\"\\nEVALUATOR\")\n    print(\"-\" * 40)\n    conf = ev.get(\"confidence\", None)\n    print(f\"  Confidence  : {conf if conf is not None else 'N/A'}\")\n    print(f\"  Missing     : {ev.get('missing_aspects', [])}\")\n\n# ‚îÄ‚îÄ Expansion output ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nif ex:\n    print(\"\\nEXPANSION\")\n    print(\"-\" * 40)\n    print(f\"  Mode             : {ex.get('mode', mode)}\")\n    if ex.get(\"mode\") == \"online\":\n        print(f\"  ArXiv query      : {ex.get('query', 'N/A')}\")\n        print(f\"  Papers downloaded: {ex.get('papers_downloaded', 0)}\")\n        print(f\"  Chunks indexed   : {ex.get('chunks_added', 0)}\")\n    else:\n        print(\"  (no internet access ‚Äî workspace mode)\")\n    print(f\"  Saturated        : {ex.get('saturated', True)}\")\n\n# ‚îÄ‚îÄ Future ideas ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nideas = result.get(\"future_ideas\", [])\nlabel = (\n    \"FUTURE RESEARCH IDEAS  ‚ö†Ô∏è  (generated from LLM knowledge ‚Äî no paper evidence)\"\n    if result.get(\"insufficient_data\") else\n    \"FUTURE RESEARCH IDEAS\"\n)\nif ideas:\n    print(f\"\\n{label}\")\n    print(\"-\" * 40)\n    for i, idea in enumerate(ideas, 1):\n        print(f\"  {i}. {idea}\")"
  },
  {
   "cell_type": "markdown",
   "id": "a37587e2",
   "metadata": {},
   "source": [
    "## 13. How to Customise\n",
    "\n",
    "| What | Where |\n",
    "|------|-------|\n",
    "| **Switch mode** | Set `SEARCH_MODE = \"online\"` or `\"workspace\"` in **Section 11** |\n",
    "| Change research goal | `RESEARCH_GOAL` in **Section 11** |\n",
    "| Add local PDFs | Drop files into `./agent_workspace/papers_local/` and re-run **Section 10** |\n",
    "| Swap LLM | Replace `ChatGroq` in **Section 1** with any LangChain `BaseChatModel` |\n",
    "| Add a 5th agent | Define `my_agent(state)`, add to `AGENT_REGISTRY`, any agent can set `next_agent = 'MyAgent'` |\n",
    "| Change online sufficiency | Edit `sufficient = confidence >= 0.75 and len(sources) >= 4` in `evaluator_agent` |\n",
    "| Change workspace sufficiency | Edit `sufficient = (confidence >= 0.50) or (len(sources) >= 2)` in `evaluator_agent` |\n",
    "| More ArXiv papers | Increase `max_results` in `expansion_agent` |\n",
    "\n",
    "---\n",
    "\n",
    "### Mode comparison\n",
    "\n",
    "```\n",
    "ONLINE mode flow:\n",
    "  Planner ‚Üí Researcher(RAG) ‚Üí Evaluator\n",
    "                                  ‚îÇ\n",
    "                          confident? ‚îÄ‚îÄYES‚îÄ‚îÄ‚ñ∫ END\n",
    "                                  ‚îÇ\n",
    "                                  NO\n",
    "                                  ‚îÇ\n",
    "                            Expansion(ArXiv download)\n",
    "                                  ‚îÇ\n",
    "                           new papers? ‚îÄ‚îÄYES‚îÄ‚îÄ‚ñ∫ Researcher (loop)\n",
    "                                  ‚îÇ\n",
    "                                  NO (saturated)\n",
    "                                  ‚îÇ\n",
    "                            generate ideas ‚Üí END\n",
    "\n",
    "WORKSPACE mode flow:\n",
    "  Planner ‚Üí Researcher(RAG) ‚Üí Evaluator\n",
    "                                  ‚îÇ\n",
    "                      any answer? ‚îÄ‚îÄYES‚îÄ‚îÄ‚ñ∫ END\n",
    "                                  ‚îÇ\n",
    "                                  NO (empty DB)\n",
    "                                  ‚îÇ\n",
    "                          Expansion(ideas only) ‚Üí END\n",
    "```\n",
    "\n",
    "### Why the flow is not hard-coded\n",
    "\n",
    "```python\n",
    "# ‚ùå  Hard-coded (bad):  Planner ‚Üí Researcher ‚Üí Evaluator ‚Üí Expansion\n",
    "g.add_edge(\"planner\",    \"researcher\")\n",
    "g.add_edge(\"researcher\", \"evaluator\")\n",
    "g.add_edge(\"evaluator\",  \"expansion\")\n",
    "\n",
    "# ‚úÖ  Dynamic (this notebook): ONE node, data-driven routing\n",
    "g.add_conditional_edges(\"agent_executor\", route, {\"LOOP\": \"agent_executor\", \"END\": END})\n",
    "# Each agent sets state['next_agent'] ‚Üí routing happens at runtime\n",
    "# Mode behaviour is embedded in the agent functions, not in graph edges\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}