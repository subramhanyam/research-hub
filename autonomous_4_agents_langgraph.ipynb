{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62ff70b7",
   "metadata": {},
   "source": [
    "# 4 Autonomous Research Agents â€” LangGraph Agentic Graph\n",
    "\n",
    "**Architecture:** A single LangGraph executor node routes dynamically at runtime.  \n",
    "The flow is **never hard-coded** â€” each agent decides `next_agent` in its output.\n",
    "\n",
    "| Agent | Role |\n",
    "|-------|------|\n",
    "| **Planner** | Understands goal, breaks into sub-questions, decides who runs next |\n",
    "| **Researcher** | RAG retrieval from vector DB, grounded answers + sources |\n",
    "| **Evaluator** | Scores confidence, detects gaps, routes to Expansion or END |\n",
    "| **Expansion** | ArXiv search (online mode) **or** idea generation only (workspace mode) |\n",
    "\n",
    "---\n",
    "\n",
    "## Two Operating Modes\n",
    "\n",
    "| Mode | `search_mode` value | Expansion behaviour |\n",
    "|------|--------------------|--------------------|\n",
    "| **Online** | `\"online\"` | Searches ArXiv, downloads & indexes new papers, loops back to Researcher |\n",
    "| **Workspace-only** | `\"workspace\"` | Never touches the internet â€” uses only local PDFs already indexed |\n",
    "\n",
    "Set `SEARCH_MODE` in **Section 11** before running.\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚          LangGraph StateGraph           â”‚\n",
    "  â”‚                                         â”‚\n",
    "  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\n",
    "  â”‚   â”‚       agent_executor         â”‚â—„â”€â”   â”‚\n",
    "  â”‚   â”‚  (single dynamic dispatcher) â”‚  â”‚   â”‚\n",
    "  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚\n",
    "  â”‚              â”‚                      â”‚   â”‚\n",
    "  â”‚         route(state)                â”‚   â”‚\n",
    "  â”‚         /         \\                 â”‚   â”‚\n",
    "  â”‚      LOOP          END              â”‚   â”‚\n",
    "  â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "  AGENT_REGISTRY maps name â†’ function at runtime:\n",
    "    'Planner'    â†’ planner_agent(state)\n",
    "    'Researcher' â†’ researcher_agent(state)\n",
    "    'Evaluator'  â†’ evaluator_agent(state)\n",
    "    'Expansion'  â†’ expansion_agent(state)   â† behaviour controlled by search_mode\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88b4da2",
   "metadata": {},
   "source": [
    "## 0. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6442b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q langgraph langchain langchain-core langchain-community langchain-groq \\\n",
    "#               langchain-openai langchain-chroma chromadb openai \\\n",
    "#               pypdf arxiv python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c28b3ba",
   "metadata": {},
   "source": [
    "## 1. Imports & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbfd5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, TypedDict\n",
    "\n",
    "import arxiv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_groq import ChatGroq\n",
    "load_dotenv()\n",
    "\n",
    "# â”€â”€ LLM (Groq â€” swap for any LangChain ChatModel) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# â”€â”€ Workspace directories â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "WORK_DIR   = Path(\"./agent_workspace\")\n",
    "PDF_DIR    = WORK_DIR / \"papers_local\"       # drop your PDFs here\n",
    "ARXIV_DIR  = WORK_DIR / \"papers_arxiv\"       # Expansion agent saves here\n",
    "CHROMA_DIR = WORK_DIR / \"chroma_db\"\n",
    "\n",
    "for d in [WORK_DIR, PDF_DIR, ARXIV_DIR, CHROMA_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“ Workspace:\", WORK_DIR.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae76d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm.invoke(\"Write a 10 line story\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11c34cc",
   "metadata": {},
   "source": [
    "## 2. Shared Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee05c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ JSON parsing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def _parse_json(text: str) -> dict:\n",
    "    t = text.strip()\n",
    "    if t.startswith(\"```\"):\n",
    "        t = t.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "    return json.loads(t)\n",
    "\n",
    "\n",
    "def _llm_json(prompt: str, fallback: dict) -> dict:\n",
    "    \"\"\"Call the LLM and parse JSON output. Return fallback on any error.\"\"\"\n",
    "    if llm is None:\n",
    "        return fallback\n",
    "    try:\n",
    "        raw = llm.invoke([HumanMessage(content=prompt)]).content\n",
    "        return _parse_json(raw)\n",
    "    except Exception as e:\n",
    "        print(f\"  [LLM parse error] {e}\")\n",
    "        return fallback\n",
    "\n",
    "\n",
    "# â”€â”€ Vector DB (Chroma + OpenAI embeddings) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def build_vectordb():\n",
    "    \"\"\"Instantiate (or reuse) the persistent Chroma vector store.\"\"\"\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    try:\n",
    "        from langchain_chroma import Chroma\n",
    "    except ImportError:\n",
    "        from langchain_community.vectorstores import Chroma  # type: ignore\n",
    "\n",
    "    emb = OpenAIEmbeddings(model=os.getenv(\"OPENAI_EMBED_MODEL\", \"text-embedding-3-small\"))\n",
    "    vdb = Chroma(\n",
    "        collection_name=\"autonomous_4agents\",\n",
    "        embedding_function=emb,\n",
    "        persist_directory=str(CHROMA_DIR),\n",
    "    )\n",
    "    return vdb\n",
    "\n",
    "\n",
    "def ingest_dirs(vdb, dirs: list[Path]) -> int:\n",
    "    \"\"\"Load all PDFs from dirs, chunk them, and upsert into the vector store.\"\"\"\n",
    "    pdfs = [p for d in dirs for p in d.rglob(\"*.pdf\")]\n",
    "    docs: list[Document] = []\n",
    "    for p in pdfs:\n",
    "        try:\n",
    "            pages = PyPDFLoader(str(p)).load()\n",
    "            for pg in pages:\n",
    "                pg.metadata[\"source\"] = str(p)\n",
    "            docs.extend(pages)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not docs:\n",
    "        return 0\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1_200, chunk_overlap=200)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    ids = [\n",
    "        hashlib.md5(\n",
    "            f\"{c.metadata.get('source')}|{c.metadata.get('page',-1)}|{c.page_content[:80]}\"\n",
    "            .encode()\n",
    "        ).hexdigest()\n",
    "        for c in chunks\n",
    "    ]\n",
    "    if chunks:\n",
    "        vdb.add_documents(chunks, ids=ids)\n",
    "    return len(chunks)\n",
    "\n",
    "\n",
    "def retrieve(vdb, query: str, k: int = 8) -> list[Document]:\n",
    "    return vdb.similarity_search(query, k=k)\n",
    "\n",
    "\n",
    "# â”€â”€ ArXiv helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def arxiv_expand(query: str, max_results: int = 4) -> list[Path]:\n",
    "    \"\"\"Search ArXiv, download PDFs, return file paths.\"\"\"\n",
    "    client = arxiv.Client(page_size=max_results, delay_seconds=3, num_retries=3)\n",
    "    search = arxiv.Search(\n",
    "        query=query, max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance,\n",
    "    )\n",
    "    saved = []\n",
    "    for r in client.results(search):\n",
    "        try:\n",
    "            fp = r.download_pdf(dirpath=str(ARXIV_DIR), filename=f\"{r.get_short_id()}.pdf\")\n",
    "            saved.append(Path(fp))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return saved\n",
    "\n",
    "\n",
    "print(\"âœ… Helpers ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d36827",
   "metadata": {},
   "source": [
    "## 3. State Schema\n",
    "\n",
    "One `TypedDict` carries everything through the graph.  \n",
    "`current_agent` tells the executor **which** agent to run next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2698e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict, total=False):\n",
    "    # â”€â”€ Core â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    goal: str                   # research question\n",
    "    vdb: Any                    # Chroma vector store (passed by reference)\n",
    "    current_agent: str          # which agent is active RIGHT NOW\n",
    "    next_agent: str             # where to route after this agent\n",
    "    iterations: int             # expansion loop counter\n",
    "    max_iterations: int         # hard cap on expansion loops\n",
    "\n",
    "    # â”€â”€ Mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    search_mode: str            # \"online\"  â†’ ArXiv search allowed\n",
    "                                # \"workspace\" â†’ use only local/already-indexed PDFs\n",
    "\n",
    "    # â”€â”€ Per-agent outputs (accumulated in state) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    planner:    dict            # {subtasks, next_agent}\n",
    "    researcher: dict            # {answer, sources, next_agent}\n",
    "    evaluator:  dict            # {confidence, missing_aspects, next_agent}\n",
    "    expansion:  dict            # {query, papers_downloaded, chunks_added, saturated, ideas, mode}\n",
    "\n",
    "    # â”€â”€ Final outputs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    final_answer: str\n",
    "    final_sources: list\n",
    "    future_ideas: list\n",
    "    trace: list                 # execution log [\"Planner â†’ Researcher\", ...]\n",
    "\n",
    "    # â”€â”€ Insufficiency tracking â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    insufficient_data: bool     # True when the system could not gather enough evidence\n",
    "    insufficiency_reason: str   # human-readable explanation of why data was insufficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd276948",
   "metadata": {},
   "source": [
    "## 4. Agent 1 â€” Planner (Lightweight Brain)\n",
    "\n",
    "- Understands the research goal  \n",
    "- Breaks it into sub-questions  \n",
    "- Decides who runs next (always `Researcher`)  \n",
    "- **Does NOT search** â€” pure reasoning only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac817b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def planner_agent(state: AgentState) -> dict:\n",
    "    goal = state[\"goal\"]\n",
    "    print(f\"  ğŸ§  [Planner] Goal: {goal[:80]}...\")\n",
    "\n",
    "    fallback = {\n",
    "        \"subtasks\": [\"model architecture\", \"training datasets\", \"evaluation benchmarks\"],\n",
    "        \"next_agent\": \"Researcher\",\n",
    "    }\n",
    "    prompt = f\"\"\"You are the Planner Agent.\n",
    "Research goal: {goal}\n",
    "\n",
    "Your job:\n",
    "1. Understand the goal deeply.\n",
    "2. Break it into 3-5 focused sub-questions.\n",
    "3. Do NOT search â€” only plan.\n",
    "4. Always route next to Researcher.\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\"subtasks\": [\"sub-question 1\", \"sub-question 2\", \"...\"], \"next_agent\": \"Researcher\"}}\"\"\"\n",
    "\n",
    "    out = _llm_json(prompt, fallback)\n",
    "    subtasks = out.get(\"subtasks\", fallback[\"subtasks\"])\n",
    "    if not isinstance(subtasks, list):\n",
    "        subtasks = fallback[\"subtasks\"]\n",
    "\n",
    "    print(f\"  ğŸ§  [Planner] Subtasks: {subtasks}\")\n",
    "    return {\n",
    "        \"planner\": {\"subtasks\": subtasks, \"next_agent\": \"Researcher\"},\n",
    "        \"next_agent\": \"Researcher\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e4d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdb = build_vectordb()\n",
    "State = {\n",
    "    \"goal\":           \"\"\"âœ” Late disease detection\n",
    "âœ” Diagnostic human errors\n",
    "âœ” Slow medical data analysis\n",
    "âœ” Limited healthcare accessibility\"\"\",\n",
    "    \"vdb\":            vdb,\n",
    "    \"search_mode\":    \"workspace\",\n",
    "    \"current_agent\":  \"Planner\",\n",
    "    \"next_agent\":     \"Planner\",\n",
    "    \"iterations\":     0,\n",
    "    \"max_iterations\": 3,\n",
    "    \"future_ideas\":   [],\n",
    "    \"trace\":          [],\n",
    "}\n",
    "planner_agent(State)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f53e74f",
   "metadata": {},
   "source": [
    "## 5. Agent 2 â€” Researcher (RAG Agent)\n",
    "\n",
    "- Retrieves evidence from the Chroma vector DB  \n",
    "- Generates a grounded answer with source citations  \n",
    "- Routes to `Evaluator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c27e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "\n",
    "# Simple grounded RAG prompt\n",
    "researcher_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are the Researcher Agent in an autonomous multi-agent research pipeline.\n",
    "\n",
    "Your job is to answer the research goal using only the retrieved <context> documents and the plannerâ€™s subtasks.\n",
    "Rules:\n",
    "1. Ground every important claim in the provided context. Do not use outside knowledge.\n",
    "2. Do not fabricate facts, sources, page numbers, or certainty.\n",
    "3. If evidence is weak, incomplete, or missing, explicitly say so.\n",
    "4. Prioritize relevance to the goal and subtasks; avoid generic commentary.\n",
    "5. Be specific and technical, but concise.\n",
    "\n",
    "Output format (plain text, no JSON):\n",
    "- Direct Answer: A clear, integrated answer to the goal (at least 2 solid paragraphs).\n",
    "- Evidence by Subtask: Bullet points mapping key findings to subtasks.\n",
    "- Limitations / Gaps: What the context does not establish yet.\n",
    "- Source Notes: Inline citations in the form [source: <path>, page: <n or unknown>] using only retrieved documents.\n",
    "\n",
    "Success criteria:\n",
    "- Accurate, context-grounded, goal-focused synthesis.\n",
    "- Honest uncertainty handling when data is insufficient.\n",
    "\"\"\"),\n",
    "    (\"human\", \"\"\"\n",
    "Research Goal:\n",
    "{goal}\n",
    "\n",
    "Planner Subtasks:\n",
    "{subtasks}\n",
    "\n",
    "Retrieved Context (only evidence you may use):\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Instructions:\n",
    "- Answer the goal directly using only the context above.\n",
    "- Cover the subtasks where evidence exists.\n",
    "- If Supported content is missing or weak, say that explicitly.\n",
    "- Do not use outside knowledge.\n",
    "\n",
    "Return in this exact structure:\n",
    "\n",
    "Direct Answer:\n",
    "<your synthesis>\n",
    "\n",
    "Support by Subtask:\n",
    "- <subtask>: <source-grounded finding> [source: <path>, page: <n or unknown>]\n",
    "- ...\n",
    "\n",
    "Limitations / Gaps:\n",
    "- ...\n",
    "\n",
    "Source Notes:\n",
    "- [source: <path>, page: <n or unknown>] <what it supports>\n",
    "- ...\n",
    "\"\"\"),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def researcher_agent(state: dict) -> dict:\n",
    "    goal = state[\"goal\"]\n",
    "    subtasks = state.get(\"planner\", {}).get(\"subtasks\", [])\n",
    "    vdb = state[\"vdb\"]\n",
    "\n",
    "    # Retrieve for main goal + each subtask\n",
    "    retriever = vdb.as_retriever(search_kwargs={\"k\": 6})\n",
    "    docs = []\n",
    "\n",
    "    docs.extend(retriever.invoke(goal))\n",
    "    for subtask in subtasks:\n",
    "        docs.extend(retriever.invoke(subtask))\n",
    "\n",
    "    # Build context\n",
    "    context = \"\\n\\n\".join(d.page_content for d in docs)\n",
    "    print(context)\n",
    "    # Run LLM chain\n",
    "    chain = researcher_prompt | llm\n",
    "    result = chain.invoke({\n",
    "        \"goal\": goal,\n",
    "        \"subtasks\": subtasks,\n",
    "        \"context\": context\n",
    "    })\n",
    "\n",
    "    answer = result.content \n",
    "\n",
    "    sources = [\n",
    "        {\n",
    "            \"source\": d.metadata.get(\"source\"),\n",
    "            \"page\": d.metadata.get(\"page\")\n",
    "        }\n",
    "        for d in docs\n",
    "    ]\n",
    "\n",
    "    # Return both legacy and final keys for compatibility across notebook cells\n",
    "    return {\n",
    "        \"researcher\": {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": sources,\n",
    "            \"next_agent\": \"Evaluator\",\n",
    "        },\n",
    "        \"final_answer\": answer,\n",
    "        \"final_sources\": sources,\n",
    "        \"next_agent\": \"Evaluator\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7517681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\"goal\":           \"\"\"âœ” Late disease detection\n",
    "âœ” Diagnostic human errors\n",
    "âœ” Slow medical data analysis\n",
    "âœ” Limited healthcare accessibility\"\"\",\n",
    "    \"vdb\":            vdb,\n",
    "    \"search_mode\":    \"workspace\",\n",
    "    \"current_agent\":  \"Planner\",\n",
    "    \"next_agent\":     \"Planner\",\n",
    "    \"iterations\":     0,\n",
    "    \"max_iterations\": 3,\n",
    "    \"future_ideas\":   [],\n",
    "    \"trace\":          [],'planner': {'subtasks': ['Systematic literature review on late disease detection factors and diagnostic errors.',\n",
    "   'Characterize diagnostic errors by modality and error type (missed, delayed, cognitive bias).',\n",
    "   'Assemble and anonymize a multi-institution dataset capturing time-to-diagnosis, misdiagnoses, and outcomes.',\n",
    "   'Develop and validate AI-assisted screening and triage models to flag potential diseases earlier.',\n",
    "   'Design clinician decision-support tools to standardize workflows and reduce cognitive bias.',\n",
    "   'Build a scalable data analysis pipeline for EMR, imaging, and lab data to accelerate insights.',\n",
    "   'Implement data interoperability with FHIR/HL7 and standard ontologies to speed data sharing.',\n",
    "   'Prototype telemedicine and mobile health interventions to expand access in underserved areas.',\n",
    "   'Pilot interventions in rural/low-resource settings to measure impact on time-to-diagnosis and error rates.',\n",
    "   'Create privacy-preserving analytics (federated learning, differential privacy) to enable rapid analysis.',\n",
    "   'Develop real-time dashboards and alert systems to monitor diagnostic timeliness and data throughput.',\n",
    "   'Develop risk stratification models to identify patients at high risk for late diagnosis.',\n",
    "   'Assess regulatory, ethical, and equity implications and formulate actionable recommendations.'],\n",
    "  'next_agent': 'Researcher'},\n",
    " 'next_agent': 'Researcher'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dd0ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "researcher_agent(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a354826",
   "metadata": {},
   "source": [
    "## 6. Agent 3 â€” Evaluator (Critic + Analyzer)\n",
    "\n",
    "- Judges whether the answer is sufficient  \n",
    "- Scores confidence (0â€“1)  \n",
    "- Detects missing aspects  \n",
    "- **Online mode:** routes to `END` if confidence â‰¥ 0.75 **and** â‰¥ 4 sources, else `Expansion`  \n",
    "- **Workspace mode:** routes to `END` once the vector DB has been exhausted (no new chunks possible), accepting a lower confidence threshold (â‰¥ 0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bab78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Evaluator prompt template â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "_evaluator_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are the Evaluator Agent.\n",
    "\n",
    "Research goal: {goal}\n",
    "Operating mode: {mode}\n",
    "\n",
    "Evaluate the following answer against the goal:\n",
    "<context>\n",
    "Answer: {answer}\n",
    "Number of sources: {num_sources}\n",
    "</context>\n",
    "\n",
    "Tasks:\n",
    "1. Score confidence from 0.0 to 1.0 (how well the answer covers the goal).\n",
    "2. List specific missing aspects (if any).\n",
    "3. Routing rule â€” {mode_instruction}\n",
    "\n",
    "Return ONLY valid JSON (no markdown, no explanation):\n",
    "{{\"confidence\": 0.62, \"missing_aspects\": [\"...\", \"...\"], \"next_agent\": \"Expansion\"}}\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def evaluator_agent(state: AgentState) -> dict:\n",
    "    goal        = state[\"goal\"]\n",
    "    researcher   = state.get(\"researcher\", {}) if isinstance(state.get(\"researcher\", {}), dict) else {}\n",
    "    answer       = researcher.get(\"answer\", state.get(\"final_answer\", \"\"))\n",
    "    sources      = researcher.get(\"sources\", state.get(\"final_sources\", []))\n",
    "    mode         = state.get(\"search_mode\", \"workspace\")\n",
    "    print(f\"  ğŸ§ª [Evaluator] Mode={mode} | Judging answer ({len(sources)} sources)...\")\n",
    "\n",
    "    fallback = {\n",
    "        \"confidence\": 0.55 if len(sources) >= 3 else 0.30,\n",
    "        \"missing_aspects\": [\"experimental validation\", \"ablation studies\"],\n",
    "        \"next_agent\": \"Expansion\",\n",
    "    }\n",
    "\n",
    "    mode_instruction = (\n",
    "        \"Set next_agent to END only if answer quality is strong and coverage is high. \"\n",
    "        \"If the answer is incomplete, vague, or weakly evidenced, set next_agent to Expansion.\"\n",
    "        if mode == \"online\"\n",
    "        else\n",
    "        \"Workspace-only mode: be moderately lenient, but still avoid END if answer is vague/off-topic/unsupported. \"\n",
    "        \"Set next_agent to Expansion when quality is poor so ideas can still be generated.\"\n",
    "    )\n",
    "\n",
    "    # â”€â”€ LCEL evaluation chain: prompt | llm â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Mirrors the document_chain pattern from 1.1.2-Simpleapp\n",
    "    out = fallback\n",
    "    if llm is not None:\n",
    "        try:\n",
    "            eval_chain = _evaluator_prompt | llm\n",
    "            result = eval_chain.invoke({\n",
    "                \"goal\":             goal,\n",
    "                \"mode\":             mode,\n",
    "                \"answer\":           answer[:1500],\n",
    "                \"num_sources\":      len(sources),\n",
    "                \"mode_instruction\": mode_instruction,\n",
    "            })\n",
    "            raw = result.content.strip() if hasattr(result, \"content\") else str(result).strip()\n",
    "            # Strip markdown fences if present\n",
    "            if raw.startswith(\"```\"):\n",
    "                raw = raw.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "            parsed = json.loads(raw)\n",
    "            out = parsed\n",
    "        except Exception as e:\n",
    "            print(f\"  ğŸ§ª [Evaluator] LLM chain error: {e}\")\n",
    "\n",
    "    confidence      = float(out.get(\"confidence\", fallback[\"confidence\"]))\n",
    "    missing_aspects = out.get(\"missing_aspects\", fallback[\"missing_aspects\"])\n",
    "    if not isinstance(missing_aspects, list):\n",
    "        missing_aspects = fallback[\"missing_aspects\"]\n",
    "\n",
    "    # â”€â”€ Deterministic quality checks (prevent false \"goal achieved\" routing) â”€â”€\n",
    "    answer_clean   = answer.strip()\n",
    "    answer_len_ok  = len(answer_clean) >= 220\n",
    "    unique_sources = len({s.get(\"source\", \"\") for s in sources if isinstance(s, dict)})\n",
    "\n",
    "    stopwords = {\n",
    "        \"what\", \"which\", \"with\", \"from\", \"into\", \"that\", \"this\", \"those\", \"these\",\n",
    "        \"their\", \"there\", \"about\", \"have\", \"been\", \"being\", \"were\", \"when\", \"where\",\n",
    "        \"how\", \"can\", \"for\", \"and\", \"the\", \"are\", \"key\", \"overcome\", \"using\", \"than\",\n",
    "    }\n",
    "    goal_terms = [\n",
    "        w for w in ''.join(ch.lower() if ch.isalnum() else ' ' for ch in goal).split()\n",
    "        if len(w) >= 5 and w not in stopwords\n",
    "    ]\n",
    "    if goal_terms:\n",
    "        matches      = sum(1 for t in goal_terms if t in answer_clean.lower())\n",
    "        goal_coverage = matches / len(goal_terms)\n",
    "    else:\n",
    "        goal_coverage = 0.0\n",
    "\n",
    "    # Cap overly optimistic model confidence if grounding is weak\n",
    "    if not answer_clean or len(sources) == 0:\n",
    "        confidence = min(confidence, 0.30)\n",
    "    elif goal_coverage < 0.25 or unique_sources < 1:\n",
    "        confidence = min(confidence, 0.55)\n",
    "    elif unique_sources < 2:\n",
    "        confidence = min(confidence, 0.70)\n",
    "\n",
    "    if mode == \"workspace\":\n",
    "        sufficient = (\n",
    "            answer_len_ok\n",
    "            and len(sources) >= 1\n",
    "            and goal_coverage >= 0.30\n",
    "            and confidence >= 0.45\n",
    "        )\n",
    "    else:\n",
    "        sufficient = (\n",
    "            answer_len_ok\n",
    "            and len(sources) >= 3\n",
    "            and unique_sources >= 2\n",
    "            and goal_coverage >= 0.40\n",
    "            and confidence >= 0.72\n",
    "        )\n",
    "\n",
    "    next_agent = \"END\" if sufficient else \"Expansion\"\n",
    "\n",
    "    print(\n",
    "        \"  ğŸ§ª [Evaluator] \"\n",
    "        f\"Confidence={confidence:.2f} | Coverage={goal_coverage:.2f} | \"\n",
    "        f\"UniqueSources={unique_sources} | Sufficient={sufficient} | Next={next_agent}\"\n",
    "    )\n",
    "    return {\n",
    "        \"evaluator\": {\n",
    "            \"confidence\":      confidence,\n",
    "            \"missing_aspects\": missing_aspects,\n",
    "            \"next_agent\":      next_agent,\n",
    "        },\n",
    "        \"next_agent\": next_agent,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee34db",
   "metadata": {},
   "source": [
    "## 7. Agent 4 â€” Expansion (Explorer + Innovator)\n",
    "\n",
    "**Online mode (`search_mode = \"online\"`):**\n",
    "- Searches ArXiv for papers covering missing aspects  \n",
    "- Ingests new PDFs into the vector DB  \n",
    "- If new papers found â†’ loops back to `Researcher`  \n",
    "- If saturated (no new papers **or** max iterations reached) â†’ generates ideas â†’ `END`\n",
    "\n",
    "**Workspace-only mode (`search_mode = \"workspace\"`):**\n",
    "- Skips all internet access  \n",
    "- Goes straight to generating novel research ideas from missing aspects  \n",
    "- Always routes to `END` (no new retrieval is possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92245aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expansion_agent(state: AgentState) -> dict:\n",
    "    goal           = state[\"goal\"]\n",
    "    vdb            = state[\"vdb\"]\n",
    "    missing        = state.get(\"evaluator\", {}).get(\"missing_aspects\", [])\n",
    "    iterations     = int(state.get(\"iterations\", 0)) + 1\n",
    "    max_iterations = int(state.get(\"max_iterations\", 4))\n",
    "    mode           = state.get(\"search_mode\", \"online\")\n",
    "    # Was the last Researcher call a total blank?\n",
    "    prev_answer    = state.get(\"final_answer\", \"\")\n",
    "    prev_sources   = state.get(\"final_sources\", [])\n",
    "\n",
    "    print(f\"  ğŸš€ [Expansion] Mode={mode} | Iteration {iterations}/{max_iterations}\")\n",
    "    print(f\"  ğŸš€ [Expansion] Missing aspects: {missing}\")\n",
    "\n",
    "    # â”€â”€ Shared: generate novel research ideas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    def _generate_ideas(reason: str) -> list[str]:\n",
    "        print(f\"  ğŸš€ [Expansion] Generating ideas ({reason})...\")\n",
    "        fallback_ideas = {\n",
    "            \"ideas\": [\n",
    "                \"Design targeted experiments to address missing aspects.\",\n",
    "                \"Create stronger benchmarks with edge-case coverage.\",\n",
    "                \"Run ablation studies to isolate causal contributors.\",\n",
    "                \"Apply iterative human-in-the-loop error analysis.\",\n",
    "                \"Explore cross-domain transfer to validate generalisability.\",\n",
    "            ]\n",
    "        }\n",
    "        prompt = f\"\"\"You are the Expansion Agent.\n",
    "Research goal: {goal}\n",
    "Missing aspects: {missing}\n",
    "Reason for idea generation: {reason}\n",
    "\n",
    "Generate 4-6 concrete, novel research ideas or future directions\n",
    "that could address the missing aspects.\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\"ideas\": [\"idea 1\", \"idea 2\", \"...\"]}}\"\"\"\n",
    "        out = _llm_json(prompt, fallback_ideas)\n",
    "        raw = out.get(\"ideas\", fallback_ideas[\"ideas\"])\n",
    "        return [str(x) for x in raw] if isinstance(raw, list) else fallback_ideas[\"ideas\"]\n",
    "\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # WORKSPACE-ONLY MODE â€” no internet, ideas only\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    if mode == \"workspace\":\n",
    "        ideas = _generate_ideas(\"workspace-only mode â€” no ArXiv access\")\n",
    "\n",
    "        # Detect true insufficiency: DB was empty when Researcher ran\n",
    "        truly_insufficient = (len(prev_sources) == 0 and prev_answer == \"\")\n",
    "        if truly_insufficient:\n",
    "            insuff_reason = (\n",
    "                \"Workspace mode is active but the vector DB contains no documents. \"\n",
    "                f\"Add PDFs to {PDF_DIR.resolve()} and re-run Section 10.\"\n",
    "            )\n",
    "            print(f\"  ğŸš€ [Expansion] âš ï¸  INSUFFICIENT DATA â€” {insuff_reason}\")\n",
    "        else:\n",
    "            insuff_reason = \"\"\n",
    "\n",
    "        print(f\"  ğŸš€ [Expansion] [WORKSPACE] Generated {len(ideas)} ideas â†’ END\")\n",
    "        out_dict = {\n",
    "            \"iterations\": iterations,\n",
    "            \"expansion\": {\n",
    "                \"mode\":              \"workspace\",\n",
    "                \"query\":             None,\n",
    "                \"papers_downloaded\": 0,\n",
    "                \"chunks_added\":      0,\n",
    "                \"saturated\":         True,\n",
    "                \"ideas\":             ideas,\n",
    "                \"next_agent\":        \"END\",\n",
    "            },\n",
    "            \"future_ideas\": ideas,\n",
    "            \"next_agent\":   \"END\",\n",
    "        }\n",
    "        if truly_insufficient:\n",
    "            out_dict[\"insufficient_data\"]     = True\n",
    "            out_dict[\"insufficiency_reason\"]  = insuff_reason\n",
    "        return out_dict\n",
    "\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # ONLINE MODE â€” ArXiv search + ingest\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    query      = f\"{goal} {' '.join(missing[:3])}\"\n",
    "    downloaded = arxiv_expand(query, max_results=4)\n",
    "    added      = ingest_dirs(vdb, [ARXIV_DIR]) if downloaded else 0\n",
    "    print(f\"  ğŸš€ [Expansion] [ONLINE] Downloaded {len(downloaded)} papers | Indexed {added} chunks\")\n",
    "\n",
    "    saturated  = (added == 0) or (iterations >= max_iterations)\n",
    "    ideas      = _generate_ideas(\"ArXiv saturated\") if saturated else []\n",
    "    next_agent = \"END\" if saturated else \"Researcher\"\n",
    "\n",
    "    # Detect true insufficiency: saturated on iteration 1 with no prior answer\n",
    "    truly_insufficient = (\n",
    "        saturated\n",
    "        and iterations == 1\n",
    "        and len(downloaded) == 0\n",
    "        and len(prev_sources) == 0\n",
    "    )\n",
    "    if truly_insufficient:\n",
    "        insuff_reason = (\n",
    "            f\"ArXiv returned no papers for the query: '{query}'. \"\n",
    "            \"The topic may be too niche, phrased unusually, or ArXiv may be unreachable. \"\n",
    "            \"Try rephrasing the research goal, adding local PDFs, or checking your connection.\"\n",
    "        )\n",
    "        print(f\"  ğŸš€ [Expansion] âš ï¸  INSUFFICIENT DATA â€” {insuff_reason}\")\n",
    "    else:\n",
    "        insuff_reason = \"\"\n",
    "\n",
    "    print(f\"  ğŸš€ [Expansion] [ONLINE] Saturated={saturated} | Next={next_agent}\")\n",
    "    out_dict = {\n",
    "        \"iterations\": iterations,\n",
    "        \"expansion\": {\n",
    "            \"mode\":              \"online\",\n",
    "            \"query\":             query,\n",
    "            \"papers_downloaded\": len(downloaded),\n",
    "            \"chunks_added\":      added,\n",
    "            \"saturated\":         saturated,\n",
    "            \"ideas\":             ideas,\n",
    "            \"next_agent\":        next_agent,\n",
    "        },\n",
    "        \"future_ideas\": ideas,\n",
    "        \"next_agent\":   next_agent,\n",
    "    }\n",
    "    if truly_insufficient:\n",
    "        out_dict[\"insufficient_data\"]    = True\n",
    "        out_dict[\"insufficiency_reason\"] = insuff_reason\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83165faf",
   "metadata": {},
   "source": [
    "## 8. Dynamic Registry & Executor\n",
    "\n",
    "**No hard-coded edges between agents.**  \n",
    "A single `agent_executor` node reads `state['current_agent']`,  \n",
    "looks up the function in `AGENT_REGISTRY`, and calls it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57432e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Registry: name â†’ function â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "AGENT_REGISTRY: dict[str, Callable[[AgentState], dict]] = {\n",
    "    \"Planner\":    planner_agent,\n",
    "    \"Researcher\": researcher_agent,\n",
    "    \"Evaluator\":  evaluator_agent,\n",
    "    \"Expansion\":  expansion_agent,\n",
    "}\n",
    "\n",
    "\n",
    "# â”€â”€ Single dynamic executor node â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def agent_executor(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    The ONLY node in the graph.\n",
    "    Reads current_agent from state, dispatches to the right function,\n",
    "    then updates state with the result.\n",
    "    \"\"\"\n",
    "    # Resolve agent functions at runtime to avoid stale references when cells are re-run out of order.\n",
    "    runtime_registry: dict[str, Callable[[AgentState], dict]] = {\n",
    "        \"Planner\": planner_agent,\n",
    "        \"Researcher\": researcher_agent,\n",
    "        \"Evaluator\": evaluator_agent,\n",
    "        \"Expansion\": expansion_agent,\n",
    "    }\n",
    "\n",
    "    current = state.get(\"current_agent\", \"Planner\")\n",
    "    fn      = runtime_registry.get(current)\n",
    "\n",
    "    if fn is None:\n",
    "        print(f\"  âŒ Unknown agent: {current}\")\n",
    "        return {\n",
    "            **state,\n",
    "            \"next_agent\":    \"END\",\n",
    "            \"current_agent\": \"END\",\n",
    "            \"trace\":         state.get(\"trace\", []) + [f\"{current} -> UNKNOWN -> END\"],\n",
    "        }\n",
    "\n",
    "    # Run the agent\n",
    "    update     = fn(state)\n",
    "    next_agent = update.get(\"next_agent\", \"END\")\n",
    "    trace      = state.get(\"trace\", []) + [f\"{current} â†’ {next_agent}\"]\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        **update,\n",
    "        \"current_agent\": next_agent,\n",
    "        \"trace\":         trace,\n",
    "    }\n",
    "\n",
    "\n",
    "# â”€â”€ Conditional routing function â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def route(state: AgentState) -> str:\n",
    "    \"\"\"\n",
    "    After every agent_executor call:\n",
    "      - next_agent == 'END'  â†’ stop the graph\n",
    "      - anything else        â†’ loop back to agent_executor\n",
    "    \"\"\"\n",
    "    return \"END\" if state.get(\"next_agent\", \"END\") == \"END\" else \"LOOP\"\n",
    "\n",
    "\n",
    "print(\"âœ… Registry:\", list(AGENT_REGISTRY.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c66847d",
   "metadata": {},
   "source": [
    "## 9. Build the LangGraph\n",
    "\n",
    "```\n",
    "  START\n",
    "    â”‚\n",
    "    â–¼\n",
    " agent_executor  â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚                           â”‚\n",
    "  route(state)                  â”‚\n",
    "   /      \\                     â”‚\n",
    " END      LOOP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "Only **one node**, one **conditional edge** â€” routing is 100% data-driven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd844a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph():\n",
    "    g = StateGraph(AgentState)\n",
    "\n",
    "    # Single executor node\n",
    "    g.add_node(\"agent_executor\", agent_executor)\n",
    "\n",
    "    # Entry point\n",
    "    g.set_entry_point(\"agent_executor\")\n",
    "\n",
    "    # Dynamic routing â€” no hard-coded agent-to-agent edges\n",
    "    g.add_conditional_edges(\n",
    "        \"agent_executor\",\n",
    "        route,\n",
    "        {\n",
    "            \"LOOP\": \"agent_executor\",   # run next agent in same node\n",
    "            \"END\":  END,                # terminate graph\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return g.compile()\n",
    "\n",
    "\n",
    "graph = build_graph()\n",
    "print(\"âœ… LangGraph compiled\")\n",
    "\n",
    "# Optional: visualise the graph structure\n",
    "try:\n",
    "    from IPython.display import display, Image\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    print(\"  (graph visualisation requires pygraphviz â€” skipping)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6579e5",
   "metadata": {},
   "source": [
    "## 10. Initialise Vector DB & Ingest Local Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e86f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building vector DB...\")\n",
    "vdb = build_vectordb()\n",
    "\n",
    "# Ingest any PDFs already in the workspace\n",
    "chunks = ingest_dirs(vdb, [PDF_DIR, ARXIV_DIR])\n",
    "print(f\"âœ… Vector DB ready | {chunks} chunks indexed from existing PDFs\")\n",
    "print(f\"   Drop additional PDFs into: {PDF_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef47fc4e",
   "metadata": {},
   "source": [
    "## 11. Run the Autonomous Agent System\n",
    "\n",
    "**Choose your mode then run the cell below.**\n",
    "\n",
    "| `SEARCH_MODE` | What happens |\n",
    "|---------------|-------------|\n",
    "| `\"online\"` | Planner â†’ Researcher (local RAG) â†’ Evaluator â†’ **Expansion downloads ArXiv papers** â†’ loops until confident â†’ ideas |\n",
    "| `\"workspace\"` | Planner â†’ Researcher (local RAG only) â†’ Evaluator â†’ **Expansion generates ideas from existing knowledge** â†’ END |\n",
    "\n",
    "> **Tip â€” workspace mode:** Make sure you have PDFs in `./agent_workspace/papers_local/`  \n",
    "> and have run **Section 10** so they are indexed before starting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540965d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘                    CONFIGURE HERE                           â•‘\n",
    "# â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "# â•‘  SEARCH_MODE                                                â•‘\n",
    "# â•‘    \"online\"    â€” search ArXiv for missing papers            â•‘\n",
    "# â•‘    \"workspace\" â€” use only locally indexed PDFs              â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "SEARCH_MODE    = \"workspace\"       # â† change to \"workspace\" for offline mode\n",
    "\n",
    "RESEARCH_GOAL  = (\n",
    "    \"To accurately predict a proteinâ€™s three-dimensional (3D) structure directly from its amino acid sequence using artificial intelligence, eliminating the need for time-consuming and expensive experimental methods.\"\n",
    ")\n",
    "\n",
    "MAX_ITERATIONS = 3              # max Expansionâ†’Researcher loops (online mode only)\n",
    "\n",
    "# â”€â”€ Validate mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "assert SEARCH_MODE in (\"online\", \"workspace\"), \\\n",
    "    f\"SEARCH_MODE must be 'online' or 'workspace', got: {SEARCH_MODE!r}\"\n",
    "\n",
    "mode_label = {\n",
    "    \"online\":    \"ğŸŒ ONLINE  â€” ArXiv search enabled\",\n",
    "    \"workspace\": \"ğŸ’¾ WORKSPACE â€” local PDFs only, no internet\",\n",
    "}[SEARCH_MODE]\n",
    "\n",
    "# â”€â”€ Initial state â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "initial_state: AgentState = {\n",
    "    \"goal\":           RESEARCH_GOAL,\n",
    "    \"vdb\":            vdb,\n",
    "    \"search_mode\":    SEARCH_MODE,\n",
    "    \"current_agent\":  \"Planner\",\n",
    "    \"next_agent\":     \"Planner\",\n",
    "    \"iterations\":     0,\n",
    "    \"max_iterations\": MAX_ITERATIONS,\n",
    "    \"future_ideas\":   [],\n",
    "    \"trace\":          [],\n",
    "}\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(f\"MODE  : {mode_label}\")\n",
    "print(f\"GOAL  : {RESEARCH_GOAL}\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# â”€â”€ Execute â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "result = graph.invoke(initial_state)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"RUN COMPLETE\")\n",
    "print(\"=\" * 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dafc5c9",
   "metadata": {},
   "source": [
    "## 12. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5qg196qnyvv",
   "metadata": {},
   "source": [
    "## 12b. Full Autonomous Research Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66twxzxdig",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  FULL AUTONOMOUS RESEARCH REPORT\n",
    "#  Displays every stage of the multi-agent run in a readable format.\n",
    "#  Run this cell after Section 11 (graph.invoke) has completed.\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "SEP  = \"â•\" * 68\n",
    "SEP2 = \"â”€\" * 68\n",
    "\n",
    "def _section(title: str) -> None:\n",
    "    print(f\"\\n{SEP}\")\n",
    "    print(f\"  {title}\")\n",
    "    print(SEP)\n",
    "\n",
    "def _sub(title: str) -> None:\n",
    "    print(f\"\\n{title}\")\n",
    "    print(SEP2)\n",
    "\n",
    "# â”€â”€ 0. Header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "mode       = result.get(\"search_mode\", \"workspace\")\n",
    "mode_label = \"ğŸŒ ONLINE  â€” ArXiv search enabled\" if mode == \"online\" else \"ğŸ’¾ WORKSPACE â€” local PDFs only\"\n",
    "_section(\"AUTONOMOUS RESEARCH REPORT\")\n",
    "print(f\"  Mode  : {mode_label}\")\n",
    "print(f\"  Goal  : {result.get('goal', 'N/A')}\")\n",
    "\n",
    "# â”€â”€ 1. Insufficient-data warning (shown prominently if triggered) â”€â”€â”€â”€\n",
    "if result.get(\"insufficient_data\"):\n",
    "    reason = result.get(\"insufficiency_reason\", \"Unknown.\")\n",
    "    print(f\"\\n{'âš ï¸  ' * 17}\")\n",
    "    print(\"  âš ï¸  INSUFFICIENT DATA â€” the system could not gather enough evidence\")\n",
    "    print(f\"  Reason : {reason}\")\n",
    "    if mode == \"online\":\n",
    "        print(\"\\n  Suggestions:\")\n",
    "        print(\"    â€¢ Rephrase RESEARCH_GOAL with more specific terminology\")\n",
    "        print(\"    â€¢ Add relevant PDFs to ./agent_workspace/papers_local/\")\n",
    "        print(\"    â€¢ Increase max_results in expansion_agent (currently 4)\")\n",
    "        print(\"    â€¢ Check internet connection (ArXiv requires network access)\")\n",
    "    else:\n",
    "        print(\"\\n  Suggestions:\")\n",
    "        print(\"    â€¢ Add PDFs to ./agent_workspace/papers_local/ and re-run Section 10\")\n",
    "        print(\"    â€¢ Switch SEARCH_MODE = 'online' to let the agent fetch papers\")\n",
    "    print(f\"{'âš ï¸  ' * 17}\")\n",
    "\n",
    "# â”€â”€ 2. Execution trace â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "_sub(\"ğŸ“ EXECUTION TRACE\")\n",
    "trace = result.get(\"trace\", [])\n",
    "if trace:\n",
    "    for step in trace:\n",
    "        print(f\"   {step}\")\n",
    "else:\n",
    "    print(\"   (no trace recorded)\")\n",
    "\n",
    "# â”€â”€ 3. Planner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "_sub(\"ğŸ§  PLANNER â€” Sub-Questions\")\n",
    "subtasks = result.get(\"planner\", {}).get(\"subtasks\", [])\n",
    "if subtasks:\n",
    "    for i, t in enumerate(subtasks, 1):\n",
    "        print(f\"   {i}. {t}\")\n",
    "else:\n",
    "    print(\"   (no subtasks generated)\")\n",
    "\n",
    "# â”€â”€ 4. Researcher â€” answer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "_sub(\"ğŸ” RESEARCHER â€” Grounded Answer\")\n",
    "answer = result.get(\"final_answer\", \"\")\n",
    "if answer:\n",
    "    # Word-wrap at ~70 chars for readability\n",
    "    import textwrap\n",
    "    for line in textwrap.wrap(answer, width=70):\n",
    "        print(f\"   {line}\")\n",
    "else:\n",
    "    print(\"   âš ï¸  No answer produced â€” evidence was unavailable.\")\n",
    "\n",
    "# â”€â”€ 5. Researcher â€” sources â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "_sub(\"ğŸ” RESEARCHER â€” Sources Retrieved\")\n",
    "sources = result.get(\"final_sources\", [])\n",
    "if sources:\n",
    "    seen = set()\n",
    "    for s in sources:\n",
    "        key = (s.get(\"source\", \"unknown\"), s.get(\"page\", \"?\"))\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            print(f\"   [p.{s.get('page','?')}]  {s.get('source','unknown')}\")\n",
    "else:\n",
    "    print(\"   âš ï¸  No sources retrieved.\")\n",
    "\n",
    "# â”€â”€ 6. Evaluator â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ev = result.get(\"evaluator\", {})\n",
    "if ev:\n",
    "    _sub(\"ğŸ§ª EVALUATOR â€” Quality Assessment\")\n",
    "    conf = ev.get(\"confidence\")\n",
    "    bar_len = int((conf or 0) * 20)\n",
    "    bar = \"â–ˆ\" * bar_len + \"â–‘\" * (20 - bar_len)\n",
    "    print(f\"   Confidence  : {conf:.2f}  [{bar}]\")\n",
    "    missing = ev.get(\"missing_aspects\", [])\n",
    "    if missing:\n",
    "        print(\"   Missing     :\")\n",
    "        for m in missing:\n",
    "            print(f\"     â€¢ {m}\")\n",
    "    else:\n",
    "        print(\"   Missing     : (none â€” answer fully covers the goal)\")\n",
    "    print(f\"   Routing     : â†’ {ev.get('next_agent', 'N/A')}\")\n",
    "\n",
    "# â”€â”€ 7. Expansion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ex = result.get(\"expansion\", {})\n",
    "if ex:\n",
    "    _sub(\"ğŸš€ EXPANSION â€” Explorer / Innovator\")\n",
    "    exp_mode = ex.get(\"mode\", mode)\n",
    "    print(f\"   Mode             : {exp_mode}\")\n",
    "    if exp_mode == \"online\":\n",
    "        print(f\"   ArXiv query      : {ex.get('query', 'N/A')}\")\n",
    "        print(f\"   Papers downloaded: {ex.get('papers_downloaded', 0)}\")\n",
    "        print(f\"   Chunks indexed   : {ex.get('chunks_added', 0)}\")\n",
    "    else:\n",
    "        print(\"   (no internet access â€” workspace mode)\")\n",
    "    print(f\"   Saturated        : {ex.get('saturated', True)}\")\n",
    "\n",
    "# â”€â”€ 8. Future research ideas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ideas = result.get(\"future_ideas\", [])\n",
    "if ideas:\n",
    "    label = (\n",
    "        \"ğŸ’¡ FUTURE RESEARCH IDEAS  âš ï¸  (from LLM knowledge â€” no paper evidence)\"\n",
    "        if result.get(\"insufficient_data\")\n",
    "        else \"ğŸ’¡ FUTURE RESEARCH IDEAS\"\n",
    "    )\n",
    "    _sub(label)\n",
    "    for i, idea in enumerate(ideas, 1):\n",
    "        import textwrap\n",
    "        wrapped = textwrap.wrap(idea, width=66)\n",
    "        print(f\"   {i}. {wrapped[0]}\")\n",
    "        for cont in wrapped[1:]:\n",
    "            print(f\"      {cont}\")\n",
    "\n",
    "# â”€â”€ 9. Summary footer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "_section(\"SUMMARY\")\n",
    "total_steps   = len(trace)\n",
    "total_sources = len({s.get(\"source\",\"\") for s in sources})\n",
    "total_ideas   = len(ideas)\n",
    "conf_val      = ev.get(\"confidence\", 0.0) if ev else 0.0\n",
    "print(f\"   Agent steps    : {total_steps}\")\n",
    "print(f\"   Unique sources : {total_sources}\")\n",
    "print(f\"   Confidence     : {conf_val:.2f}\")\n",
    "print(f\"   Future ideas   : {total_ideas}\")\n",
    "print(f\"   Data status    : {'âš ï¸  INSUFFICIENT' if result.get('insufficient_data') else 'âœ… OK'}\")\n",
    "print(f\"\\n{SEP}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i2ec4p2etuh",
   "metadata": {},
   "source": [
    "## 12c. Research Brief â€” What Was Found & What Are the Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7pyaij0px",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  RESEARCH BRIEF\n",
    "#  Uses the LLM to write a concise brief covering:\n",
    "#    1. What the research found (key findings)\n",
    "#    2. Limitations (evidence gaps, missing aspects, data quality)\n",
    "#  Falls back to a rule-based brief if LLM is unavailable.\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "_brief_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a senior research analyst. Based on the autonomous research run below,\n",
    "write a concise research brief in plain English. Do NOT use JSON.\n",
    "\n",
    "â”€â”€â”€ RESEARCH INPUT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Goal        : {goal}\n",
    "Sub-questions: {subtasks}\n",
    "\n",
    "â”€â”€â”€ AGENT OUTPUTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Grounded Answer (from Researcher Agent):\n",
    "{answer}\n",
    "\n",
    "Sources used: {num_sources} document(s)\n",
    "Evaluator confidence score: {confidence}\n",
    "Missing aspects flagged by Evaluator: {missing}\n",
    "\n",
    "Future research ideas (from Expansion Agent, if any): {ideas}\n",
    "\n",
    "â”€â”€â”€ INSTRUCTIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Write the brief in exactly this structure â€” keep each section short and factual:\n",
    "\n",
    "**RESEARCH BRIEF**\n",
    "\n",
    "**Goal**\n",
    "One sentence restating what the research was trying to find out.\n",
    "\n",
    "**Key Findings**\n",
    "2â€“4 bullet points summarising what the agents actually found, grounded\n",
    "in the retrieved evidence. If evidence was weak, say so honestly.\n",
    "\n",
    "**Limitations**\n",
    "2â€“4 bullet points on what is missing, uncertain, or poorly evidenced â€”\n",
    "draw from the evaluator's missing aspects and confidence score.\n",
    "\n",
    "**Future Directions**\n",
    "1â€“3 bullet points on what should be explored next (use the ideas list\n",
    "if available, otherwise infer from the limitations).\n",
    "\n",
    "Keep the entire brief under 300 words.\"\"\"\n",
    ")\n",
    "\n",
    "# â”€â”€ Collect inputs from `result` â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "_goal       = result.get(\"goal\", \"N/A\")\n",
    "_subtasks   = result.get(\"planner\", {}).get(\"subtasks\", [])\n",
    "_answer     = result.get(\"final_answer\", \"No answer was produced.\")\n",
    "_sources    = result.get(\"final_sources\", [])\n",
    "_ev         = result.get(\"evaluator\", {})\n",
    "_confidence = _ev.get(\"confidence\", 0.0) if _ev else 0.0\n",
    "_missing    = _ev.get(\"missing_aspects\", []) if _ev else []\n",
    "_ideas      = result.get(\"future_ideas\", [])\n",
    "\n",
    "# â”€â”€ Generate brief via LLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SEP = \"â•\" * 68\n",
    "\n",
    "if llm is not None:\n",
    "    brief_chain = _brief_prompt | llm\n",
    "    _result     = brief_chain.invoke({\n",
    "        \"goal\":        _goal,\n",
    "        \"subtasks\":    \"\\n\".join(f\"  â€¢ {t}\" for t in _subtasks) or \"  (none)\",\n",
    "        \"answer\":      _answer[:2000],\n",
    "        \"num_sources\": len(_sources),\n",
    "        \"confidence\":  f\"{_confidence:.2f}\",\n",
    "        \"missing\":     \"\\n\".join(f\"  â€¢ {m}\" for m in _missing) or \"  (none flagged)\",\n",
    "        \"ideas\":       \"\\n\".join(f\"  â€¢ {i}\" for i in _ideas)   or \"  (none generated)\",\n",
    "    })\n",
    "    brief_text = _result.content.strip() if hasattr(_result, \"content\") else str(_result).strip()\n",
    "else:\n",
    "    # â”€â”€ Rule-based fallback (no LLM) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    brief_text = (\n",
    "        \"**RESEARCH BRIEF**\\n\\n\"\n",
    "        f\"**Goal**\\n{_goal}\\n\\n\"\n",
    "        \"**Key Findings**\\n\"\n",
    "        + (\"\\n\".join(f\"  â€¢ {_answer[:300]}...\") if _answer else \"  â€¢ No findings â€” evidence unavailable.\\n\")\n",
    "        + \"\\n\\n**Limitations**\\n\"\n",
    "        + (\"\\n\".join(f\"  â€¢ {m}\" for m in _missing) if _missing else \"  â€¢ Confidence too low to draw conclusions.\\n\")\n",
    "        + f\"\\n  â€¢ Confidence score: {_confidence:.2f} (below threshold)\\n\"\n",
    "        + f\"  â€¢ Only {len(_sources)} source(s) retrieved.\\n\\n\"\n",
    "        \"**Future Directions**\\n\"\n",
    "        + (\"\\n\".join(f\"  â€¢ {i}\" for i in _ideas) if _ideas else \"  â€¢ Switch to online mode and expand paper corpus.\\n\")\n",
    "    )\n",
    "\n",
    "# â”€â”€ Print â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\n{SEP}\")\n",
    "print(brief_text)\n",
    "print(f\"{SEP}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047cdc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex   = result.get(\"expansion\", {})\n",
    "mode = result.get(\"search_mode\", \"workspace\")\n",
    "\n",
    "# â”€â”€ Header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 65)\n",
    "print(f\"MODE : {'ğŸŒ ONLINE' if mode == 'online' else 'ğŸ’¾ WORKSPACE'}\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# â”€â”€ Insufficient data warning â€” shown first if triggered â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if result.get(\"insufficient_data\"):\n",
    "    reason = result.get(\"insufficiency_reason\", \"Unknown reason.\")\n",
    "    print(\"\\n\" + \"âš ï¸ \" * 20)\n",
    "    print(\"  INSUFFICIENT DATA â€” the system could not gather enough evidence\")\n",
    "    print(\"  Reason  :\", reason)\n",
    "    print()\n",
    "    if mode == \"online\":\n",
    "        print(\"  Suggestions:\")\n",
    "        print(\"    1. Rephrase RESEARCH_GOAL with more specific or standard terminology\")\n",
    "        print(\"    2. Add relevant PDFs manually to ./agent_workspace/papers_local/\")\n",
    "        print(\"       then re-run Section 10 to index them\")\n",
    "        print(\"    3. Increase max_results in expansion_agent (currently 4)\")\n",
    "        print(\"    4. Check your internet connection (ArXiv requires network access)\")\n",
    "    else:\n",
    "        print(\"  Suggestions:\")\n",
    "        print(\"    1. Add PDFs to ./agent_workspace/papers_local/ and re-run Section 10\")\n",
    "        print(\"    2. Switch to SEARCH_MODE = 'online' to let the agent fetch papers\")\n",
    "    print(\"âš ï¸ \" * 20)\n",
    "\n",
    "# â”€â”€ Execution trace â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nEXECUTION TRACE\")\n",
    "print(\"-\" * 40)\n",
    "for step in result.get(\"trace\", []):\n",
    "    print(\" â€¢\", step)\n",
    "\n",
    "# â”€â”€ Planner output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nPLANNER â€” SUB-QUESTIONS\")\n",
    "print(\"-\" * 40)\n",
    "for i, t in enumerate(result.get(\"planner\", {}).get(\"subtasks\", []), 1):\n",
    "    print(f\"  {i}. {t}\")\n",
    "\n",
    "# â”€â”€ Researcher output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nRESEARCHER â€” GROUNDED ANSWER\")\n",
    "print(\"-\" * 40)\n",
    "answer = result.get(\"final_answer\", \"\")\n",
    "if answer:\n",
    "    print(answer)\n",
    "else:\n",
    "    print(\"  âš ï¸  No answer produced â€” evidence was unavailable.\")\n",
    "\n",
    "print(\"\\nRESEARCHER â€” SOURCES\")\n",
    "sources = result.get(\"final_sources\", [])\n",
    "if sources:\n",
    "    for s in sources[:10]:\n",
    "        print(f\"  [p.{s.get('page', '?')}] {s.get('source', 'unknown')}\")\n",
    "else:\n",
    "    print(\"  âš ï¸  No sources retrieved.\")\n",
    "\n",
    "# â”€â”€ Evaluator output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ev = result.get(\"evaluator\", {})\n",
    "if ev:\n",
    "    print(\"\\nEVALUATOR\")\n",
    "    print(\"-\" * 40)\n",
    "    conf = ev.get(\"confidence\", None)\n",
    "    print(f\"  Confidence  : {conf if conf is not None else 'N/A'}\")\n",
    "    print(f\"  Missing     : {ev.get('missing_aspects', [])}\")\n",
    "\n",
    "# â”€â”€ Expansion output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if ex:\n",
    "    print(\"\\nEXPANSION\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Mode             : {ex.get('mode', mode)}\")\n",
    "    if ex.get(\"mode\") == \"online\":\n",
    "        print(f\"  ArXiv query      : {ex.get('query', 'N/A')}\")\n",
    "        print(f\"  Papers downloaded: {ex.get('papers_downloaded', 0)}\")\n",
    "        print(f\"  Chunks indexed   : {ex.get('chunks_added', 0)}\")\n",
    "    else:\n",
    "        print(\"  (no internet access â€” workspace mode)\")\n",
    "    print(f\"  Saturated        : {ex.get('saturated', True)}\")\n",
    "\n",
    "# â”€â”€ Future ideas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ideas = result.get(\"future_ideas\", [])\n",
    "label = (\n",
    "    \"FUTURE RESEARCH IDEAS  âš ï¸  (generated from LLM knowledge â€” no paper evidence)\"\n",
    "    if result.get(\"insufficient_data\") else\n",
    "    \"FUTURE RESEARCH IDEAS\"\n",
    ")\n",
    "if ideas:\n",
    "    print(f\"\\n{label}\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, idea in enumerate(ideas, 1):\n",
    "        print(f\"  {i}. {idea}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37587e2",
   "metadata": {},
   "source": [
    "## 13. How to Customise\n",
    "\n",
    "| What | Where |\n",
    "|------|-------|\n",
    "| **Switch mode** | Set `SEARCH_MODE = \"online\"` or `\"workspace\"` in **Section 11** |\n",
    "| Change research goal | `RESEARCH_GOAL` in **Section 11** |\n",
    "| Add local PDFs | Drop files into `./agent_workspace/papers_local/` and re-run **Section 10** |\n",
    "| Swap LLM | Replace `ChatGroq` in **Section 1** with any LangChain `BaseChatModel` |\n",
    "| Add a 5th agent | Define `my_agent(state)`, add to `AGENT_REGISTRY`, any agent can set `next_agent = 'MyAgent'` |\n",
    "| Change online sufficiency | Edit `sufficient = confidence >= 0.75 and len(sources) >= 4` in `evaluator_agent` |\n",
    "| Change workspace sufficiency | Edit `sufficient = (confidence >= 0.50) or (len(sources) >= 2)` in `evaluator_agent` |\n",
    "| More ArXiv papers | Increase `max_results` in `expansion_agent` |\n",
    "\n",
    "---\n",
    "\n",
    "### Mode comparison\n",
    "\n",
    "```\n",
    "ONLINE mode flow:\n",
    "  Planner â†’ Researcher(RAG) â†’ Evaluator\n",
    "                                  â”‚\n",
    "                          confident? â”€â”€YESâ”€â”€â–º END\n",
    "                                  â”‚\n",
    "                                  NO\n",
    "                                  â”‚\n",
    "                            Expansion(ArXiv download)\n",
    "                                  â”‚\n",
    "                           new papers? â”€â”€YESâ”€â”€â–º Researcher (loop)\n",
    "                                  â”‚\n",
    "                                  NO (saturated)\n",
    "                                  â”‚\n",
    "                            generate ideas â†’ END\n",
    "\n",
    "WORKSPACE mode flow:\n",
    "  Planner â†’ Researcher(RAG) â†’ Evaluator\n",
    "                                  â”‚\n",
    "                      any answer? â”€â”€YESâ”€â”€â–º END\n",
    "                                  â”‚\n",
    "                                  NO (empty DB)\n",
    "                                  â”‚\n",
    "                          Expansion(ideas only) â†’ END\n",
    "```\n",
    "\n",
    "### Why the flow is not hard-coded\n",
    "\n",
    "```python\n",
    "# âŒ  Hard-coded (bad):  Planner â†’ Researcher â†’ Evaluator â†’ Expansion\n",
    "g.add_edge(\"planner\",    \"researcher\")\n",
    "g.add_edge(\"researcher\", \"evaluator\")\n",
    "g.add_edge(\"evaluator\",  \"expansion\")\n",
    "\n",
    "# âœ…  Dynamic (this notebook): ONE node, data-driven routing\n",
    "g.add_conditional_edges(\"agent_executor\", route, {\"LOOP\": \"agent_executor\", \"END\": END})\n",
    "# Each agent sets state['next_agent'] â†’ routing happens at runtime\n",
    "# Mode behaviour is embedded in the agent functions, not in graph edges\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
